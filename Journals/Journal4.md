# Journal for Week 4 (due 9/21)

1. Reflect on the software workshop: are there any areas of Overleaf that youâ€™re still
struggling with?

I am not sure if I would say I am struggling with any section. I think the software workshop helped introduce a lot of cool concepts such as how to make images
and I also really liked the part about minipages. I feel like getting good at Overleaf and LaTeX is just going to be something that takes time and practice (and 
Googling). I don't really think I will be creating a lot of images from tikZ because of the nature of my project so that is convenient. The area I need the most 
work in is beamer but I think with practice I will be able to figure it out.

2. What did you do this week? Please be as specific as possible by filling out the time
sheet provided below.

I didn't have as much time during the week to focus on research so I did all mine over this past weekend. I had a really productive
time though. The first thing I did was look into what makes a good neural network (accuracy and efficiency). I found this kaggle
website https://www.kaggle.com/cdeotte/how-to-choose-cnn-architecture-mnist. It was really helpful and gave good insights. Next,
I worked on making a confusion matrix which took a while but I got it. Lastly, I plotted feature maps of each convolution and
pooling layer.

The time layout was:

Saturday 6:00pm-7:00pm - Researched CNN's

Sunday 10:00-11:30 - Worked on Confusion Matrix

Sunday 12:30-1:30 - Worked on Fine Tuning CNN

Sunday 3:30-5:30 - Worked on Feature Maps


3. What questions do you have for me? What do you need from me to progress further?

I do not really have questions for you. I am excited to talk about the confusion matrix, the website I found, and the feature maps.
I think all of these are really cool and allow me to be able to learn more about my CNN models.

4. What is your plan for next week?

For next week, I will probably continue to work on fine tuning my CNN. For some reason I still cannot get it to 99%+ which is what
I am shooting for. I might also look into batch normalization and data augmentation.
