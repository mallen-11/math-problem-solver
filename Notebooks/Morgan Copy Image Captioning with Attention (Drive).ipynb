{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8xUzM901Sjpe"
   },
   "source": [
    "# Image Captioning with Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 23950,
     "status": "ok",
     "timestamp": 1605553608791,
     "user": {
      "displayName": "Morgan Allen",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjaNMWmr2TlLrY_MBSDquiSqhgTuK8vCNzY4tLH=s64",
      "userId": "12026828979516915336"
     },
     "user_tz": 360
    },
    "id": "rMgFop76Sjpf",
    "outputId": "b2cf6200-462f-466c-c514-b99318169097"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "from PIL import Image\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import plot_confusion_matrix, confusion_matrix\n",
    "\n",
    "from tensorflow.keras.layers import Dense, Flatten, Conv2D, MaxPooling2D, Dropout, BatchNormalization\n",
    "from tensorflow.keras import Model\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow import image\n",
    "\n",
    "import itertools\n",
    "\n",
    "tf.random.set_seed(1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CqP6UkWTSjpj"
   },
   "source": [
    "Loading the Data. The data consist of pictures and labels of linear functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "executionInfo": {
     "elapsed": 976,
     "status": "ok",
     "timestamp": 1605553609779,
     "user": {
      "displayName": "Morgan Allen",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjaNMWmr2TlLrY_MBSDquiSqhgTuK8vCNzY4tLH=s64",
      "userId": "12026828979516915336"
     },
     "user_tz": 360
    },
    "id": "08AiJm9zSjpk"
   },
   "outputs": [],
   "source": [
    "import cv2\n",
    "import os\n",
    "\n",
    "def load_images_from_folder(folder, n_imgs=-1):\n",
    "    images = []\n",
    "    image_nums = []\n",
    "    for filename in os.listdir(folder)[:n_imgs]:\n",
    "        img = cv2.imread(os.path.join(folder,filename))\n",
    "        if img is not None:\n",
    "            images.append(img)\n",
    "            image_nums.append(filename.strip('.png'))\n",
    "    return images, image_nums\n",
    "\n",
    "folder=\"../../linear_fcns/images\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "executionInfo": {
     "elapsed": 45955,
     "status": "ok",
     "timestamp": 1605554061902,
     "user": {
      "displayName": "Morgan Allen",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjaNMWmr2TlLrY_MBSDquiSqhgTuK8vCNzY4tLH=s64",
      "userId": "12026828979516915336"
     },
     "user_tz": 360
    },
    "id": "vBBKDvA8Sjpm"
   },
   "outputs": [],
   "source": [
    "images, fnames = load_images_from_folder(folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "WYkOrKFFSjpp"
   },
   "outputs": [],
   "source": [
    "labels = pd.read_csv('../../linear_fcns/data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "JoZjytqMWTiR"
   },
   "outputs": [],
   "source": [
    "labels['img_number'] = labels['filename'].apply(lambda x: x.split('/')[-1].strip('.png'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 204
    },
    "executionInfo": {
     "elapsed": 217,
     "status": "ok",
     "timestamp": 1605395495815,
     "user": {
      "displayName": "Morgan Allen",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjaNMWmr2TlLrY_MBSDquiSqhgTuK8vCNzY4tLH=s64",
      "userId": "12026828979516915336"
     },
     "user_tz": 360
    },
    "id": "vC3wCKPxWQaC",
    "outputId": "902efffe-60ca-411b-f507-a329adf0f01a"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>latex</th>\n",
       "      <th>filename</th>\n",
       "      <th>img_number</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0a+1=2</td>\n",
       "      <td>linear_fcns/images/0.png</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0b+1=2</td>\n",
       "      <td>linear_fcns/images/1.png</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0c+1=2</td>\n",
       "      <td>linear_fcns/images/2.png</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0d+1=2</td>\n",
       "      <td>linear_fcns/images/3.png</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0e+1=2</td>\n",
       "      <td>linear_fcns/images/4.png</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    latex                  filename img_number\n",
       "0  0a+1=2  linear_fcns/images/0.png          0\n",
       "1  0b+1=2  linear_fcns/images/1.png          1\n",
       "2  0c+1=2  linear_fcns/images/2.png          2\n",
       "3  0d+1=2  linear_fcns/images/3.png          3\n",
       "4  0e+1=2  linear_fcns/images/4.png          4"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "DC-tLPESWAy5"
   },
   "outputs": [],
   "source": [
    "label_array = labels[labels['img_number'].isin(fnames)]['latex'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 202,
     "status": "ok",
     "timestamp": 1605395498172,
     "user": {
      "displayName": "Morgan Allen",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjaNMWmr2TlLrY_MBSDquiSqhgTuK8vCNzY4tLH=s64",
      "userId": "12026828979516915336"
     },
     "user_tz": 360
    },
    "id": "UBW7zYvSXC-m",
    "outputId": "b24000a7-1008-4eb5-ab5f-34d9a3ca1569"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['0a+1=2', '0b+1=2', '0c+1=2', ..., '9x+8=7', '9y+8=7', '9z+8=7'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_array"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zXR3M4QBSjpv"
   },
   "source": [
    "For the labels we need to add start and end tokens so the model can recognize what to write."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "NEzxzBX8Sjpv"
   },
   "outputs": [],
   "source": [
    "label_array = [f'\\t{la}\\n' for la in label_array]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HHztSmQ_Sjpy"
   },
   "source": [
    "Let's reshape the images so we can plot them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 244,
     "status": "ok",
     "timestamp": 1605395503599,
     "user": {
      "displayName": "Morgan Allen",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjaNMWmr2TlLrY_MBSDquiSqhgTuK8vCNzY4tLH=s64",
      "userId": "12026828979516915336"
     },
     "user_tz": 360
    },
    "id": "9XdF3xaGSjpy",
    "outputId": "9b633ce9-2750-4b1c-e926-9f2b75a9d58f"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(18719, 72, 360, 3)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "images = np.array(images)\n",
    "images = 255 - images\n",
    "# images = tf.image.rgb_to_grayscale(images)\n",
    "images = np.array(images)\n",
    "images.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 135
    },
    "executionInfo": {
     "elapsed": 318,
     "status": "ok",
     "timestamp": 1605395504243,
     "user": {
      "displayName": "Morgan Allen",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjaNMWmr2TlLrY_MBSDquiSqhgTuK8vCNzY4tLH=s64",
      "userId": "12026828979516915336"
     },
     "user_tz": 360
    },
    "id": "bHBLN_NNSjp1",
    "outputId": "2ca0ea3a-90d8-40db-816c-59860d93059a"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7ffcba005f10>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(images[0].squeeze())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fQYvxF90Sjp3"
   },
   "source": [
    "Now we can process them and prepare them for inceptionv3 which is transfer learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "pE87-f9ESjp4"
   },
   "outputs": [],
   "source": [
    "img_mini = tf.image.resize_with_pad(images, 299, 299)\n",
    "img_mini = tf.keras.applications.inception_v3.preprocess_input(img_mini)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "hhGFXB4cfoQP"
   },
   "outputs": [],
   "source": [
    "test_img = img_mini[2222]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "s-8n1FtqSjp8"
   },
   "source": [
    "Now we can load the features of the inceptionv3 model as seen below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 303
    },
    "executionInfo": {
     "elapsed": 305,
     "status": "ok",
     "timestamp": 1605395508912,
     "user": {
      "displayName": "Morgan Allen",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjaNMWmr2TlLrY_MBSDquiSqhgTuK8vCNzY4tLH=s64",
      "userId": "12026828979516915336"
     },
     "user_tz": 360
    },
    "id": "8uSNNnvdSjp6",
    "outputId": "c123a164-46bf-4b86-9368-f105029f9f0e"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7ffc8bda33d0>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(tf.squeeze(img_mini[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "hgB8p8KZSjp8"
   },
   "outputs": [],
   "source": [
    "image_model = tf.keras.applications.InceptionV3(include_top=False,\n",
    "                                                weights='imagenet')\n",
    "new_input = image_model.input\n",
    "hidden_layer = image_model.layers[-1].output\n",
    "\n",
    "image_features_extract_model = tf.keras.Model(new_input, hidden_layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "llWCIcu3Sjp_"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50.72460103034973\n",
      "51.30096983909607\n",
      "0.5763688087463379\n",
      "32.94227910041809\n",
      "33.442338943481445\n",
      "0.5000598430633545\n",
      "31.193147897720337\n",
      "31.715389013290405\n",
      "0.5222411155700684\n",
      "31.30634117126465\n",
      "31.833388090133667\n",
      "0.5270469188690186\n",
      "33.8474280834198\n",
      "34.46813416481018\n",
      "0.6207060813903809\n",
      "38.175379276275635\n",
      "38.81036639213562\n",
      "0.6349871158599854\n",
      "54.89798927307129\n",
      "55.9008252620697\n",
      "1.002835988998413\n",
      "43.53984880447388\n",
      "44.42450976371765\n",
      "0.8846609592437744\n",
      "44.18565392494202\n",
      "44.76571202278137\n",
      "0.5800580978393555\n",
      "43.511730909347534\n",
      "44.17490100860596\n",
      "0.6631700992584229\n",
      "42.181363105773926\n",
      "42.75337815284729\n",
      "0.5720150470733643\n",
      "42.62173080444336\n",
      "43.15097713470459\n",
      "0.5292463302612305\n",
      "38.34805417060852\n",
      "38.88285827636719\n",
      "0.534804105758667\n",
      "38.540546894073486\n",
      "39.252058029174805\n",
      "0.7115111351013184\n",
      "38.34740090370178\n",
      "38.91440987586975\n",
      "0.5670089721679688\n",
      "42.16173696517944\n",
      "42.710243225097656\n",
      "0.5485062599182129\n",
      "39.796581983566284\n",
      "40.31315207481384\n",
      "0.5165700912475586\n",
      "42.389888048172\n",
      "42.93109083175659\n",
      "0.5412027835845947\n",
      "42.173336029052734\n",
      "42.792792081832886\n",
      "0.6194560527801514\n",
      "42.26581621170044\n",
      "42.819987058639526\n",
      "0.5541708469390869\n",
      "40.189704179763794\n",
      "40.73347020149231\n",
      "0.5437660217285156\n",
      "38.42650389671326\n",
      "38.94666600227356\n",
      "0.5201621055603027\n",
      "42.64317798614502\n",
      "43.21207118034363\n",
      "0.5688931941986084\n",
      "38.39243292808533\n",
      "38.926363945007324\n",
      "0.5339310169219971\n",
      "37.51536011695862\n",
      "38.04795813560486\n",
      "0.5325980186462402\n",
      "39.159934997558594\n",
      "39.76122999191284\n",
      "0.601294994354248\n",
      "43.66971516609192\n",
      "44.21692514419556\n",
      "0.5472099781036377\n",
      "38.02654194831848\n",
      "38.56500220298767\n",
      "0.5384602546691895\n",
      "38.68402695655823\n",
      "39.22279715538025\n",
      "0.5387701988220215\n",
      "38.05313730239868\n",
      "38.59943914413452\n",
      "0.5463018417358398\n",
      "38.49421787261963\n",
      "39.03471374511719\n",
      "0.5404958724975586\n",
      "41.33461022377014\n",
      "41.87031698226929\n",
      "0.5357067584991455\n",
      "38.411322832107544\n",
      "39.03279900550842\n",
      "0.6214761734008789\n",
      "40.692949056625366\n",
      "41.2526478767395\n",
      "0.5596988201141357\n",
      "39.87389397621155\n",
      "40.38111090660095\n",
      "0.5072169303894043\n",
      "37.86826181411743\n",
      "38.381964921951294\n",
      "0.5137031078338623\n",
      "40.260982036590576\n",
      "40.80486488342285\n",
      "0.5438828468322754\n",
      "38.486233949661255\n",
      "39.039307832717896\n",
      "0.5530738830566406\n",
      "33.8907573223114\n",
      "34.40262413024902\n",
      "0.5118668079376221\n",
      "32.03303098678589\n",
      "32.531002044677734\n",
      "0.4979710578918457\n",
      "31.394626140594482\n",
      "31.93024206161499\n",
      "0.5356159210205078\n",
      "30.55158495903015\n",
      "31.067726850509644\n",
      "0.5161418914794922\n",
      "30.801984786987305\n",
      "31.310775756835938\n",
      "0.5087909698486328\n",
      "33.539408922195435\n",
      "34.08480620384216\n",
      "0.5453972816467285\n",
      "30.4238178730011\n",
      "30.947799682617188\n",
      "0.5239818096160889\n",
      "30.465848922729492\n",
      "30.97019386291504\n",
      "0.5043449401855469\n",
      "30.47678017616272\n",
      "30.995747089385986\n",
      "0.5189669132232666\n",
      "32.3837788105011\n",
      "32.88824677467346\n",
      "0.5044679641723633\n",
      "34.8406879901886\n",
      "35.332541942596436\n",
      "0.4918539524078369\n",
      "30.558008670806885\n",
      "31.118355989456177\n",
      "0.560347318649292\n",
      "36.733936071395874\n",
      "37.25297403335571\n",
      "0.5190379619598389\n",
      "32.580533027648926\n",
      "33.26589608192444\n",
      "0.6853630542755127\n",
      "31.04195785522461\n",
      "31.538779973983765\n",
      "0.4968221187591553\n",
      "30.496663093566895\n",
      "31.025241136550903\n",
      "0.5285780429840088\n",
      "30.264518976211548\n",
      "30.802987098693848\n",
      "0.5384681224822998\n",
      "31.681462049484253\n",
      "32.19476509094238\n",
      "0.5133030414581299\n",
      "31.70973014831543\n",
      "32.23714828491211\n",
      "0.5274181365966797\n",
      "31.104784965515137\n",
      "31.63972783088684\n",
      "0.5349428653717041\n",
      "32.95052909851074\n",
      "33.48035502433777\n",
      "0.5298259258270264\n",
      "31.02581286430359\n",
      "31.56083106994629\n",
      "0.5350182056427002\n",
      "32.08358192443848\n",
      "32.62060213088989\n",
      "0.537020206451416\n",
      "31.378220081329346\n",
      "31.911638975143433\n",
      "0.5334188938140869\n",
      "11.294892072677612\n",
      "11.511332988739014\n",
      "0.21644091606140137\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "\n",
    "\n",
    "# Feel free to change batch_size according to your system configuration\n",
    "image_dataset = tf.data.Dataset.from_tensor_slices((img_mini, label_array)).batch(300)\n",
    "c = 1\n",
    "\n",
    "for img, label in image_dataset:\n",
    "    start = time.time()\n",
    "    batch_features = image_features_extract_model(img)\n",
    "    batch_features = tf.reshape(batch_features,\n",
    "                              (batch_features.shape[0], -1, batch_features.shape[3]))\n",
    "  \n",
    "    p = f'Batch Features/batch_features{c}'\n",
    "    c = c+1 \n",
    "     #path_of_feature = p.numpy().decode(\"utf-8\")\n",
    "    end_process = time.time()\n",
    "    np.save(p, batch_features)\n",
    "    end_save = time.time()\n",
    "    print(end_process - start)\n",
    "    print(end_save - start)\n",
    "    print(end_save-end_process)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "iKq5_-oTP22q"
   },
   "outputs": [],
   "source": [
    "img_load = np.load('Batch Features/batch_features1.npy')\n",
    "for i in range(2,61):\n",
    "    img_add = np.load(f'Batch Features/batch_features{i}.npy')\n",
    "    img_load = np.concatenate((img_load, img_add))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 273,
     "status": "ok",
     "timestamp": 1605396800358,
     "user": {
      "displayName": "Morgan Allen",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjaNMWmr2TlLrY_MBSDquiSqhgTuK8vCNzY4tLH=s64",
      "userId": "12026828979516915336"
     },
     "user_tz": 360
    },
    "id": "js8_OsgYQLM3",
    "outputId": "cda0393f-c083-4bfd-96f5-e5e67e122071"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(18000, 64, 2048)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img_load.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dywlI7LUSjqC"
   },
   "source": [
    "### Tokenizing the labels\n",
    "\n",
    "Now we can pad the labels to make sure they are all the same length."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "xPgRuUgBbnPJ"
   },
   "outputs": [],
   "source": [
    "# Find the maximum length of any caption in our dataset\n",
    "def calc_max_length(tensor):\n",
    "    return max(len(t) for t in tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "Gipm5btLaNTb"
   },
   "outputs": [],
   "source": [
    "# Choose the top 5000 words from the vocabulary\n",
    "top_k = 5000\n",
    "tokenizer = tf.keras.preprocessing.text.Tokenizer(num_words=top_k,\n",
    "                                                  oov_token=\"<unk>\",\n",
    "                                                  filters=' ',\n",
    "                                                  char_level=True)\n",
    "tokenizer.fit_on_texts(label_array)\n",
    "train_seqs = tokenizer.texts_to_sequences(label_array)\n",
    "\n",
    "# Padding\n",
    "tokenizer.word_index['<pad>'] = 0\n",
    "tokenizer.index_word[0] = '<pad>'\n",
    "\n",
    "# Create the tokenized vectors\n",
    "train_seqs = tokenizer.texts_to_sequences(label_array)\n",
    "\n",
    "# Pad each vector to the max_length of the captions\n",
    "# If you do not provide a max_length value, pad_sequences calculates it automatically\n",
    "cap_vector = tf.keras.preprocessing.sequence.pad_sequences(train_seqs, padding='post')\n",
    "\n",
    "# Calculates the max_length, which is used to store the attention weights\n",
    "max_length = calc_max_length(train_seqs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 203,
     "status": "ok",
     "timestamp": 1605396804461,
     "user": {
      "displayName": "Morgan Allen",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjaNMWmr2TlLrY_MBSDquiSqhgTuK8vCNzY4tLH=s64",
      "userId": "12026828979516915336"
     },
     "user_tz": 360
    },
    "id": "YfFg62gAcaL9",
    "outputId": "2f57f655-d469-4ca3-c192-68bd3b2e5087"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'num_words': 5000,\n",
       " 'filters': ' ',\n",
       " 'lower': True,\n",
       " 'split': ' ',\n",
       " 'char_level': True,\n",
       " 'oov_token': '<unk>',\n",
       " 'document_count': 18719,\n",
       " 'word_counts': '{\"\\\\t\": 18719, \"0\": 5615, \"a\": 720, \"+\": 18719, \"1\": 5616, \"=\": 18719, \"2\": 5616, \"\\\\n\": 18719, \"b\": 720, \"c\": 719, \"d\": 720, \"e\": 720, \"f\": 720, \"g\": 720, \"h\": 720, \"i\": 720, \"j\": 720, \"k\": 720, \"l\": 720, \"m\": 720, \"n\": 720, \"o\": 720, \"p\": 720, \"q\": 720, \"r\": 720, \"s\": 720, \"t\": 720, \"u\": 720, \"v\": 720, \"w\": 720, \"x\": 720, \"y\": 720, \"z\": 720, \"3\": 5616, \"4\": 5615, \"5\": 5616, \"6\": 5616, \"7\": 5616, \"8\": 5616, \"9\": 5615}',\n",
       " 'word_docs': '{\"2\": 5616, \"\\\\n\": 18719, \"=\": 18719, \"+\": 18719, \"a\": 720, \"1\": 5616, \"0\": 5615, \"\\\\t\": 18719, \"b\": 720, \"c\": 719, \"d\": 720, \"e\": 720, \"f\": 720, \"g\": 720, \"h\": 720, \"i\": 720, \"j\": 720, \"k\": 720, \"l\": 720, \"m\": 720, \"n\": 720, \"o\": 720, \"p\": 720, \"q\": 720, \"r\": 720, \"s\": 720, \"t\": 720, \"u\": 720, \"v\": 720, \"w\": 720, \"x\": 720, \"y\": 720, \"z\": 720, \"3\": 5616, \"4\": 5615, \"5\": 5616, \"6\": 5616, \"7\": 5616, \"8\": 5616, \"9\": 5615}',\n",
       " 'index_docs': '{\"7\": 5616, \"5\": 18719, \"4\": 18719, \"3\": 18719, \"16\": 720, \"6\": 5616, \"13\": 5615, \"2\": 18719, \"17\": 720, \"41\": 719, \"18\": 720, \"19\": 720, \"20\": 720, \"21\": 720, \"22\": 720, \"23\": 720, \"24\": 720, \"25\": 720, \"26\": 720, \"27\": 720, \"28\": 720, \"29\": 720, \"30\": 720, \"31\": 720, \"32\": 720, \"33\": 720, \"34\": 720, \"35\": 720, \"36\": 720, \"37\": 720, \"38\": 720, \"39\": 720, \"40\": 720, \"8\": 5616, \"14\": 5615, \"9\": 5616, \"10\": 5616, \"11\": 5616, \"12\": 5616, \"15\": 5615}',\n",
       " 'index_word': '{\"1\": \"<unk>\", \"2\": \"\\\\t\", \"3\": \"+\", \"4\": \"=\", \"5\": \"\\\\n\", \"6\": \"1\", \"7\": \"2\", \"8\": \"3\", \"9\": \"5\", \"10\": \"6\", \"11\": \"7\", \"12\": \"8\", \"13\": \"0\", \"14\": \"4\", \"15\": \"9\", \"16\": \"a\", \"17\": \"b\", \"18\": \"d\", \"19\": \"e\", \"20\": \"f\", \"21\": \"g\", \"22\": \"h\", \"23\": \"i\", \"24\": \"j\", \"25\": \"k\", \"26\": \"l\", \"27\": \"m\", \"28\": \"n\", \"29\": \"o\", \"30\": \"p\", \"31\": \"q\", \"32\": \"r\", \"33\": \"s\", \"34\": \"t\", \"35\": \"u\", \"36\": \"v\", \"37\": \"w\", \"38\": \"x\", \"39\": \"y\", \"40\": \"z\", \"41\": \"c\", \"0\": \"<pad>\"}',\n",
       " 'word_index': '{\"<unk>\": 1, \"\\\\t\": 2, \"+\": 3, \"=\": 4, \"\\\\n\": 5, \"1\": 6, \"2\": 7, \"3\": 8, \"5\": 9, \"6\": 10, \"7\": 11, \"8\": 12, \"0\": 13, \"4\": 14, \"9\": 15, \"a\": 16, \"b\": 17, \"d\": 18, \"e\": 19, \"f\": 20, \"g\": 21, \"h\": 22, \"i\": 23, \"j\": 24, \"k\": 25, \"l\": 26, \"m\": 27, \"n\": 28, \"o\": 29, \"p\": 30, \"q\": 31, \"r\": 32, \"s\": 33, \"t\": 34, \"u\": 35, \"v\": 36, \"w\": 37, \"x\": 38, \"y\": 39, \"z\": 40, \"c\": 41, \"<pad>\": 0}'}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.get_config()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 261,
     "status": "ok",
     "timestamp": 1605396805750,
     "user": {
      "displayName": "Morgan Allen",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjaNMWmr2TlLrY_MBSDquiSqhgTuK8vCNzY4tLH=s64",
      "userId": "12026828979516915336"
     },
     "user_tz": 360
    },
    "id": "GhqB3K30dMEo",
    "outputId": "aa4eee8d-77a9-4033-c9e3-2fea59b698fa"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 417,
     "status": "ok",
     "timestamp": 1605396806687,
     "user": {
      "displayName": "Morgan Allen",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjaNMWmr2TlLrY_MBSDquiSqhgTuK8vCNzY4tLH=s64",
      "userId": "12026828979516915336"
     },
     "user_tz": 360
    },
    "id": "QKIAGkIIb5ty",
    "outputId": "ddfd8187-4314-40ca-f60f-abd639d21457"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t0a+1=2\n",
      " -> [2, 13, 16, 3, 6, 4, 7, 5]\n"
     ]
    }
   ],
   "source": [
    "print(f'{label_array[0]} -> {train_seqs[0]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 177,
     "status": "ok",
     "timestamp": 1605396807076,
     "user": {
      "displayName": "Morgan Allen",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjaNMWmr2TlLrY_MBSDquiSqhgTuK8vCNzY4tLH=s64",
      "userId": "12026828979516915336"
     },
     "user_tz": 360
    },
    "id": "wWBs6Njidhy4",
    "outputId": "c72990bc-4ddb-4444-ecfe-89622e4d5e17"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2, 13, 16, 3, 6, 4, 7, 5]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "char_to_int_map = tokenizer.get_config()['word_index']\n",
    "char_to_int_map = json.loads(char_to_int_map)\n",
    "[char_to_int_map[c] for c in label_array[0]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "B_UEKfswSjqQ"
   },
   "source": [
    "### Train Test Split\n",
    "\n",
    "This kind of makes a train test split but I don't think it is exactly what we want."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5U9-UPeDSjqS"
   },
   "source": [
    "For the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "5lwnj-e6ebxE"
   },
   "outputs": [],
   "source": [
    "img_train = img_mini[:14000]\n",
    "img_test = img_mini[14000:]\n",
    "img_name = cap_vector[:14000]\n",
    "img_name_test = cap_vector[14000:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 199,
     "status": "ok",
     "timestamp": 1605396811202,
     "user": {
      "displayName": "Morgan Allen",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjaNMWmr2TlLrY_MBSDquiSqhgTuK8vCNzY4tLH=s64",
      "userId": "12026828979516915336"
     },
     "user_tz": 360
    },
    "id": "PE0q0nk2N02f",
    "outputId": "53af654d-e8a7-43ea-e221-61ba5f534dd8"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([14000, 299, 299, 3])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "id": "dmak8HTlSjqT"
   },
   "outputs": [],
   "source": [
    "# Feel free to change these parameters according to your system's configuration\n",
    "\n",
    "BATCH_SIZE = 1\n",
    "BUFFER_SIZE = 20\n",
    "embedding_dim = 16\n",
    "units = 32\n",
    "vocab_size = top_k + 1\n",
    "num_steps = len(img_train) // BATCH_SIZE\n",
    "# Shape of the vector extracted from InceptionV3 is (64, 2048)\n",
    "# These two variables represent that vector shape\n",
    "features_shape = 2048\n",
    "attention_features_shape = 64"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "G8xkSSS7Sjqb"
   },
   "source": [
    "## Model\n",
    "\n",
    "Below we are defining the attention, the encoder and the decoder. The encoder is just a fully connected layer from the features already extracted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "id": "f7L_cEJzSjqc"
   },
   "outputs": [],
   "source": [
    "class BahdanauAttention(tf.keras.Model):\n",
    "    def __init__(self, units):\n",
    "        super(BahdanauAttention, self).__init__()\n",
    "        self.W1 = tf.keras.layers.Dense(units)\n",
    "        self.W2 = tf.keras.layers.Dense(units)\n",
    "        self.V = tf.keras.layers.Dense(1)\n",
    "\n",
    "    def call(self, features, hidden):\n",
    "        # features(CNN_encoder output) shape == (batch_size, 64, embedding_dim)\n",
    "\n",
    "        # hidden shape == (batch_size, hidden_size)\n",
    "        # hidden_with_time_axis shape == (batch_size, 1, hidden_size)\n",
    "        hidden_with_time_axis = tf.expand_dims(hidden, 1)\n",
    "\n",
    "        # attention_hidden_layer shape == (batch_size, 64, units)\n",
    "        attention_hidden_layer = (tf.nn.tanh(self.W1(features) +\n",
    "                                             self.W2(hidden_with_time_axis)))\n",
    "\n",
    "        # score shape == (batch_size, 64, 1)\n",
    "        # This gives you an unnormalized score for each image feature.\n",
    "        score = self.V(attention_hidden_layer)\n",
    "\n",
    "        # attention_weights shape == (batch_size, 64, 1)\n",
    "        attention_weights = tf.nn.softmax(score, axis=1)\n",
    "\n",
    "        # context_vector shape after sum == (batch_size, hidden_size)\n",
    "        context_vector = attention_weights * features\n",
    "        context_vector = tf.reduce_sum(context_vector, axis=1)\n",
    "\n",
    "        return context_vector, attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "id": "Eij3DZsYSjqe"
   },
   "outputs": [],
   "source": [
    "class CNN_Encoder(tf.keras.Model):\n",
    "    # Since you have already extracted the features and dumped it using pickle\n",
    "    # This encoder passes those features through a Fully connected layer\n",
    "    def __init__(self, embedding_dim):\n",
    "        super(CNN_Encoder, self).__init__()\n",
    "        # shape after fc == (batch_size, 64, embedding_dim)\n",
    "        self.fc = tf.keras.layers.Dense(embedding_dim)\n",
    "\n",
    "    def call(self, x):\n",
    "        x = self.fc(x)\n",
    "        #print(f'This is x: {x}')\n",
    "        x = tf.nn.relu(x)\n",
    "        #print(f'This is x: {x}')\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "id": "dLl75ZtPSjqh"
   },
   "outputs": [],
   "source": [
    "class RNN_Decoder(tf.keras.Model):\n",
    "    def __init__(self, embedding_dim, units, vocab_size):\n",
    "        super(RNN_Decoder, self).__init__()\n",
    "        self.units = units\n",
    "\n",
    "        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
    "        self.gru = tf.keras.layers.GRU(self.units,\n",
    "                                       return_sequences=True,\n",
    "                                       return_state=True,\n",
    "                                       recurrent_initializer='glorot_uniform')\n",
    "        self.fc1 = tf.keras.layers.Dense(self.units)\n",
    "        self.fc2 = tf.keras.layers.Dense(vocab_size)\n",
    "\n",
    "        self.attention = BahdanauAttention(self.units)\n",
    "\n",
    "    def call(self, x, features, hidden):\n",
    "        # defining attention as a separate model\n",
    "        context_vector, attention_weights = self.attention(features, hidden)\n",
    "        #print('decoder attention complete')\n",
    "        #print(x.shape)\n",
    "\n",
    "        # x shape after passing through embedding == (batch_size, 1, embedding_dim)\n",
    "        x = self.embedding(x)\n",
    "        #print('decoder embedding complete')\n",
    "        #print(f'x.shape = {x.shape}')\n",
    "        #print(f'context_vector.shape = {context_vector.shape}')\n",
    "\n",
    "        # x shape after concatenation == (batch_size, 1, embedding_dim + hidden_size)\n",
    "        x = tf.concat([tf.expand_dims(context_vector, 1), x], axis=-1)\n",
    "        #print('decoder embedding + context vector complete')\n",
    "\n",
    "        # passing the concatenated vector to the GRU\n",
    "        output, state = self.gru(x)\n",
    "        #print('decoder gru complete')\n",
    "\n",
    "        # shape == (batch_size, max_length, hidden_size)\n",
    "        x = self.fc1(output)\n",
    "        #print('decoder fc1 complete')\n",
    "\n",
    "        # x shape == (batch_size * max_length, hidden_size)\n",
    "        x = tf.reshape(x, (-1, x.shape[2]))\n",
    "        #print('decoder reshape complete')\n",
    "\n",
    "        # output shape == (batch_size * max_length, vocab)\n",
    "        x = self.fc2(x)\n",
    "        #print('decoder fc2 complete')\n",
    "\n",
    "        return x, state, attention_weights\n",
    "\n",
    "    def reset_state(self, batch_size):\n",
    "        return tf.zeros((batch_size, self.units))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "id": "quKzY4IeSjqj"
   },
   "outputs": [],
   "source": [
    "encoder = CNN_Encoder(embedding_dim)\n",
    "decoder = RNN_Decoder(embedding_dim, units, vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "id": "kI5g_iYrSjql"
   },
   "outputs": [],
   "source": [
    "optimizer = tf.keras.optimizers.Adam()\n",
    "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(\n",
    "    from_logits=True, reduction='none')\n",
    "\n",
    "def loss_function(real, pred):\n",
    "    mask = tf.math.logical_not(tf.math.equal(real, 0))\n",
    "    loss_ = loss_object(real, pred)\n",
    "\n",
    "    mask = tf.cast(mask, dtype=loss_.dtype)\n",
    "    loss_ *= mask\n",
    "\n",
    "    return tf.reduce_mean(loss_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "id": "MASAvbV3Sjqn"
   },
   "outputs": [],
   "source": [
    "checkpoint_path = \"./checkpoints/train\"\n",
    "ckpt = tf.train.Checkpoint(encoder=encoder,\n",
    "                           decoder=decoder,\n",
    "                           optimizer = optimizer)\n",
    "ckpt_manager = tf.train.CheckpointManager(ckpt, checkpoint_path, max_to_keep=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "id": "8yQjrGWkSjqo"
   },
   "outputs": [],
   "source": [
    "start_epoch = 0\n",
    "if ckpt_manager.latest_checkpoint:\n",
    "      start_epoch = int(ckpt_manager.latest_checkpoint.split('-')[-1])\n",
    "      # restoring the latest checkpoint in checkpoint_path\n",
    "      ckpt.restore(ckpt_manager.latest_checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "id": "IE7pUZDGSjqq"
   },
   "outputs": [],
   "source": [
    "# adding this in a separate cell because if you run the training cell\n",
    "# many times, the loss_plot array will be reset\n",
    "loss_plot = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "id": "JHXjLxYuSjqs"
   },
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def train_step(img_tensor, target):\n",
    "    loss = 0\n",
    "\n",
    "          # initializing the hidden state for each batch\n",
    "          # because the captions are not related from image to image\n",
    "    hidden = decoder.reset_state(batch_size=target.shape[0])\n",
    "    #print(f'hidden complete: {hidden}')\n",
    "\n",
    "    # Create a vector of all \\t indices to indicate the start of prediction\n",
    "    dec_input = tf.expand_dims([tokenizer.word_index['\\t']] * target.shape[0], 1)\n",
    "    #print(f'dec_input complete: {dec_input}')\n",
    "\n",
    "    with tf.GradientTape() as tape:\n",
    "        features = encoder(img_tensor)\n",
    "        #print('encoder complete')\n",
    "        #print(f'features.shape = {features.shape}')\n",
    "\n",
    "        # iterate through timesteps to predict the i'th character\n",
    "        for i in range(1, target.shape[1]):\n",
    "            # passing the features through the decoder\n",
    "            predictions, hidden, _ = decoder(dec_input, features, hidden)\n",
    "            #print('decoder complete')\n",
    "\n",
    "            loss += loss_function(target[:, i], predictions)\n",
    "\n",
    "            # using teacher forcing\n",
    "            dec_input = tf.expand_dims(target[:, i], 1)\n",
    "\n",
    "    total_loss = (loss / int(target.shape[1]))\n",
    "\n",
    "    trainable_variables = encoder.trainable_variables + decoder.trainable_variables\n",
    "\n",
    "    gradients = tape.gradient(loss, trainable_variables)\n",
    "\n",
    "    optimizer.apply_gradients(zip(gradients, trainable_variables))\n",
    "\n",
    "    return loss, total_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "new = cap_vector[:18000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "id": "hKZ-gQg4hmoZ"
   },
   "outputs": [],
   "source": [
    "image_dataset_encoded = tf.data.Dataset.from_tensor_slices((img_load, new)).batch(300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 7322,
     "status": "ok",
     "timestamp": 1605397126121,
     "user": {
      "displayName": "Morgan Allen",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjaNMWmr2TlLrY_MBSDquiSqhgTuK8vCNzY4tLH=s64",
      "userId": "12026828979516915336"
     },
     "user_tz": 360
    },
    "id": "vZPAuWz8Sjqu",
    "outputId": "cacc9dc1-aae4-4391-dba4-b28245e0d454"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Batch 0 Loss 7.4536\n",
      "Epoch 1 Batch 1 Loss 7.4470\n",
      "Epoch 1 Batch 2 Loss 7.4381\n",
      "Epoch 1 Batch 3 Loss 7.4222\n",
      "Epoch 1 Batch 4 Loss 7.3940\n",
      "Epoch 1 Batch 5 Loss 7.3611\n",
      "Epoch 1 Batch 6 Loss 7.3244\n",
      "Epoch 1 Batch 7 Loss 7.2860\n",
      "Epoch 1 Batch 8 Loss 7.2551\n",
      "Epoch 1 Batch 9 Loss 7.2027\n",
      "Epoch 1 Batch 10 Loss 7.1550\n",
      "Epoch 1 Batch 11 Loss 7.0977\n",
      "Epoch 1 Batch 12 Loss 7.0223\n",
      "Epoch 1 Batch 13 Loss 6.9588\n",
      "Epoch 1 Batch 14 Loss 6.8851\n",
      "Epoch 1 Batch 15 Loss 6.8163\n",
      "Epoch 1 Batch 16 Loss 6.7497\n",
      "Epoch 1 Batch 17 Loss 6.6726\n",
      "Epoch 1 Batch 18 Loss 6.5924\n",
      "Epoch 1 Batch 19 Loss 6.5223\n",
      "Epoch 1 Batch 20 Loss 6.4188\n",
      "Epoch 1 Batch 21 Loss 6.3362\n",
      "Epoch 1 Batch 22 Loss 6.2364\n",
      "Epoch 1 Batch 23 Loss 6.1236\n",
      "Epoch 1 Batch 24 Loss 6.0119\n",
      "Epoch 1 Batch 25 Loss 5.8862\n",
      "Epoch 1 Batch 26 Loss 5.7615\n",
      "Epoch 1 Batch 27 Loss 5.6131\n",
      "Epoch 1 Batch 28 Loss 5.4904\n",
      "Epoch 1 Batch 29 Loss 5.3399\n",
      "Epoch 1 Batch 30 Loss 5.1891\n",
      "Epoch 1 Batch 31 Loss 5.0223\n",
      "Epoch 1 Batch 32 Loss 4.9058\n",
      "Epoch 1 Batch 33 Loss 4.7327\n",
      "Epoch 1 Batch 34 Loss 4.6074\n",
      "Epoch 1 Batch 35 Loss 4.4736\n",
      "Epoch 1 Batch 36 Loss 4.3343\n",
      "Epoch 1 Batch 37 Loss 4.1911\n",
      "Epoch 1 Batch 38 Loss 4.1042\n",
      "Epoch 1 Batch 39 Loss 3.9380\n",
      "Epoch 1 Batch 40 Loss 3.8280\n",
      "Epoch 1 Batch 41 Loss 3.7100\n",
      "Epoch 1 Batch 42 Loss 3.6024\n",
      "Epoch 1 Batch 43 Loss 3.4804\n",
      "Epoch 1 Batch 44 Loss 3.3966\n",
      "Epoch 1 Batch 45 Loss 3.2703\n",
      "Epoch 1 Batch 46 Loss 3.1757\n",
      "Epoch 1 Batch 47 Loss 3.0755\n",
      "Epoch 1 Batch 48 Loss 2.9974\n",
      "Epoch 1 Batch 49 Loss 2.9318\n",
      "Epoch 1 Batch 50 Loss 2.8840\n",
      "Epoch 1 Batch 51 Loss 2.8341\n",
      "Epoch 1 Batch 52 Loss 2.7857\n",
      "Epoch 1 Batch 53 Loss 2.7215\n",
      "Epoch 1 Batch 54 Loss 2.6714\n",
      "Epoch 1 Batch 55 Loss 2.6482\n",
      "Epoch 1 Batch 56 Loss 2.6583\n",
      "Epoch 1 Batch 57 Loss 2.6891\n",
      "Epoch 1 Batch 58 Loss 2.6500\n",
      "Epoch 1 Batch 59 Loss 2.6515\n",
      "14000\n",
      "Epoch 1 Loss 0.021960\n",
      "Time taken for 1 epoch 63.45790386199951 sec\n",
      "\n",
      "Epoch 2 Batch 0 Loss 2.6735\n",
      "Epoch 2 Batch 1 Loss 2.6392\n",
      "Epoch 2 Batch 2 Loss 2.6492\n",
      "Epoch 2 Batch 3 Loss 2.6250\n",
      "Epoch 2 Batch 4 Loss 2.5814\n",
      "Epoch 2 Batch 5 Loss 2.5706\n",
      "Epoch 2 Batch 6 Loss 2.6015\n",
      "Epoch 2 Batch 7 Loss 2.6788\n",
      "Epoch 2 Batch 8 Loss 2.6636\n",
      "Epoch 2 Batch 9 Loss 2.6580\n",
      "Epoch 2 Batch 10 Loss 2.6292\n",
      "Epoch 2 Batch 11 Loss 2.5791\n",
      "Epoch 2 Batch 12 Loss 2.5817\n",
      "Epoch 2 Batch 13 Loss 2.5950\n",
      "Epoch 2 Batch 14 Loss 2.6706\n",
      "Epoch 2 Batch 15 Loss 2.6536\n",
      "Epoch 2 Batch 16 Loss 2.6400\n",
      "Epoch 2 Batch 17 Loss 2.6146\n",
      "Epoch 2 Batch 18 Loss 2.5756\n",
      "Epoch 2 Batch 19 Loss 2.5782\n",
      "Epoch 2 Batch 20 Loss 2.5898\n",
      "Epoch 2 Batch 21 Loss 2.6614\n",
      "Epoch 2 Batch 22 Loss 2.6447\n",
      "Epoch 2 Batch 23 Loss 2.6224\n",
      "Epoch 2 Batch 24 Loss 2.6071\n",
      "Epoch 2 Batch 25 Loss 2.5868\n",
      "Epoch 2 Batch 26 Loss 2.5739\n",
      "Epoch 2 Batch 27 Loss 2.5873\n",
      "Epoch 2 Batch 28 Loss 2.6535\n",
      "Epoch 2 Batch 29 Loss 2.6367\n",
      "Epoch 2 Batch 30 Loss 2.6136\n",
      "Epoch 2 Batch 31 Loss 2.6315\n",
      "Epoch 2 Batch 32 Loss 2.5818\n",
      "Epoch 2 Batch 33 Loss 2.5719\n",
      "Epoch 2 Batch 34 Loss 2.5869\n",
      "Epoch 2 Batch 35 Loss 2.6448\n",
      "Epoch 2 Batch 36 Loss 2.6297\n",
      "Epoch 2 Batch 37 Loss 2.6277\n",
      "Epoch 2 Batch 38 Loss 2.6356\n",
      "Epoch 2 Batch 39 Loss 2.5720\n",
      "Epoch 2 Batch 40 Loss 2.5631\n",
      "Epoch 2 Batch 41 Loss 2.5821\n",
      "Epoch 2 Batch 42 Loss 2.6307\n",
      "Epoch 2 Batch 43 Loss 2.6317\n",
      "Epoch 2 Batch 44 Loss 2.6600\n",
      "Epoch 2 Batch 45 Loss 2.6382\n",
      "Epoch 2 Batch 46 Loss 2.5685\n",
      "Epoch 2 Batch 47 Loss 2.5609\n",
      "Epoch 2 Batch 48 Loss 2.5835\n",
      "Epoch 2 Batch 49 Loss 2.6251\n",
      "Epoch 2 Batch 50 Loss 2.6665\n",
      "Epoch 2 Batch 51 Loss 2.6461\n",
      "Epoch 2 Batch 52 Loss 2.6266\n",
      "Epoch 2 Batch 53 Loss 2.5614\n",
      "Epoch 2 Batch 54 Loss 2.5520\n",
      "Epoch 2 Batch 55 Loss 2.5812\n",
      "Epoch 2 Batch 56 Loss 2.6527\n",
      "Epoch 2 Batch 57 Loss 2.6604\n",
      "Epoch 2 Batch 58 Loss 2.6324\n",
      "Epoch 2 Batch 59 Loss 2.6156\n",
      "14000\n",
      "Epoch 2 Loss 0.011211\n",
      "Time taken for 1 epoch 39.39979100227356 sec\n",
      "\n",
      "Epoch 3 Batch 0 Loss 2.6755\n",
      "Epoch 3 Batch 1 Loss 2.6447\n",
      "Epoch 3 Batch 2 Loss 2.6410\n",
      "Epoch 3 Batch 3 Loss 2.5947\n",
      "Epoch 3 Batch 4 Loss 2.5551\n",
      "Epoch 3 Batch 5 Loss 2.5540\n",
      "Epoch 3 Batch 6 Loss 2.5868\n",
      "Epoch 3 Batch 7 Loss 2.6571\n",
      "Epoch 3 Batch 8 Loss 2.6316\n",
      "Epoch 3 Batch 9 Loss 2.6243\n",
      "Epoch 3 Batch 10 Loss 2.5880\n",
      "Epoch 3 Batch 11 Loss 2.5453\n",
      "Epoch 3 Batch 12 Loss 2.5624\n",
      "Epoch 3 Batch 13 Loss 2.5832\n",
      "Epoch 3 Batch 14 Loss 2.6545\n",
      "Epoch 3 Batch 15 Loss 2.6329\n",
      "Epoch 3 Batch 16 Loss 2.6210\n",
      "Epoch 3 Batch 17 Loss 2.5873\n",
      "Epoch 3 Batch 18 Loss 2.5593\n",
      "Epoch 3 Batch 19 Loss 2.5697\n",
      "Epoch 3 Batch 20 Loss 2.5860\n",
      "Epoch 3 Batch 21 Loss 2.6585\n",
      "Epoch 3 Batch 22 Loss 2.6387\n",
      "Epoch 3 Batch 23 Loss 2.6137\n",
      "Epoch 3 Batch 24 Loss 2.5953\n",
      "Epoch 3 Batch 25 Loss 2.5767\n",
      "Epoch 3 Batch 26 Loss 2.5706\n",
      "Epoch 3 Batch 27 Loss 2.5885\n",
      "Epoch 3 Batch 28 Loss 2.6551\n",
      "Epoch 3 Batch 29 Loss 2.6306\n",
      "Epoch 3 Batch 30 Loss 2.6042\n",
      "Epoch 3 Batch 31 Loss 2.6174\n",
      "Epoch 3 Batch 32 Loss 2.5769\n",
      "Epoch 3 Batch 33 Loss 2.5681\n",
      "Epoch 3 Batch 34 Loss 2.5845\n",
      "Epoch 3 Batch 35 Loss 2.6464\n",
      "Epoch 3 Batch 36 Loss 2.6228\n",
      "Epoch 3 Batch 37 Loss 2.6200\n",
      "Epoch 3 Batch 38 Loss 2.6291\n",
      "Epoch 3 Batch 39 Loss 2.5683\n",
      "Epoch 3 Batch 40 Loss 2.5609\n",
      "Epoch 3 Batch 41 Loss 2.5821\n",
      "Epoch 3 Batch 42 Loss 2.6286\n",
      "Epoch 3 Batch 43 Loss 2.6239\n",
      "Epoch 3 Batch 44 Loss 2.6489\n",
      "Epoch 3 Batch 45 Loss 2.6307\n",
      "Epoch 3 Batch 46 Loss 2.5674\n",
      "Epoch 3 Batch 47 Loss 2.5583\n",
      "Epoch 3 Batch 48 Loss 2.5811\n",
      "Epoch 3 Batch 49 Loss 2.6214\n",
      "Epoch 3 Batch 50 Loss 2.6581\n",
      "Epoch 3 Batch 51 Loss 2.6401\n",
      "Epoch 3 Batch 52 Loss 2.6217\n",
      "Epoch 3 Batch 53 Loss 2.5570\n",
      "Epoch 3 Batch 54 Loss 2.5462\n",
      "Epoch 3 Batch 55 Loss 2.5764\n",
      "Epoch 3 Batch 56 Loss 2.6439\n",
      "Epoch 3 Batch 57 Loss 2.6518\n",
      "Epoch 3 Batch 58 Loss 2.6249\n",
      "Epoch 3 Batch 59 Loss 2.6105\n",
      "14000\n",
      "Epoch 3 Loss 0.011168\n",
      "Time taken for 1 epoch 38.33546304702759 sec\n",
      "\n",
      "Epoch 4 Batch 0 Loss 2.6586\n",
      "Epoch 4 Batch 1 Loss 2.6323\n",
      "Epoch 4 Batch 2 Loss 2.6318\n",
      "Epoch 4 Batch 3 Loss 2.5906\n",
      "Epoch 4 Batch 4 Loss 2.5479\n",
      "Epoch 4 Batch 5 Loss 2.5485\n",
      "Epoch 4 Batch 6 Loss 2.5739\n",
      "Epoch 4 Batch 7 Loss 2.6429\n",
      "Epoch 4 Batch 8 Loss 2.6207\n",
      "Epoch 4 Batch 9 Loss 2.6156\n",
      "Epoch 4 Batch 10 Loss 2.5796\n",
      "Epoch 4 Batch 11 Loss 2.5373\n",
      "Epoch 4 Batch 12 Loss 2.5574\n",
      "Epoch 4 Batch 13 Loss 2.5784\n",
      "Epoch 4 Batch 14 Loss 2.6507\n",
      "Epoch 4 Batch 15 Loss 2.6293\n",
      "Epoch 4 Batch 16 Loss 2.6173\n",
      "Epoch 4 Batch 17 Loss 2.5817\n",
      "Epoch 4 Batch 18 Loss 2.5561\n",
      "Epoch 4 Batch 19 Loss 2.5664\n",
      "Epoch 4 Batch 20 Loss 2.5832\n",
      "Epoch 4 Batch 21 Loss 2.6515\n",
      "Epoch 4 Batch 22 Loss 2.6301\n",
      "Epoch 4 Batch 23 Loss 2.6053\n",
      "Epoch 4 Batch 24 Loss 2.5864\n",
      "Epoch 4 Batch 25 Loss 2.5699\n",
      "Epoch 4 Batch 26 Loss 2.5641\n",
      "Epoch 4 Batch 27 Loss 2.5824\n",
      "Epoch 4 Batch 28 Loss 2.6466\n",
      "Epoch 4 Batch 29 Loss 2.6208\n",
      "Epoch 4 Batch 30 Loss 2.5970\n",
      "Epoch 4 Batch 31 Loss 2.6076\n",
      "Epoch 4 Batch 32 Loss 2.5707\n",
      "Epoch 4 Batch 33 Loss 2.5644\n",
      "Epoch 4 Batch 34 Loss 2.5831\n",
      "Epoch 4 Batch 35 Loss 2.6364\n",
      "Epoch 4 Batch 36 Loss 2.6139\n",
      "Epoch 4 Batch 37 Loss 2.6110\n",
      "Epoch 4 Batch 38 Loss 2.6151\n",
      "Epoch 4 Batch 39 Loss 2.5637\n",
      "Epoch 4 Batch 40 Loss 2.5597\n",
      "Epoch 4 Batch 41 Loss 2.5784\n",
      "Epoch 4 Batch 42 Loss 2.6205\n",
      "Epoch 4 Batch 43 Loss 2.6104\n",
      "Epoch 4 Batch 44 Loss 2.6263\n",
      "Epoch 4 Batch 45 Loss 2.6072\n",
      "Epoch 4 Batch 46 Loss 2.5543\n",
      "Epoch 4 Batch 47 Loss 2.5505\n",
      "Epoch 4 Batch 48 Loss 2.5722\n",
      "Epoch 4 Batch 49 Loss 2.6066\n",
      "Epoch 4 Batch 50 Loss 2.6385\n",
      "Epoch 4 Batch 51 Loss 2.6221\n",
      "Epoch 4 Batch 52 Loss 2.6016\n",
      "Epoch 4 Batch 53 Loss 2.5471\n",
      "Epoch 4 Batch 54 Loss 2.5417\n",
      "Epoch 4 Batch 55 Loss 2.5673\n",
      "Epoch 4 Batch 56 Loss 2.6274\n",
      "Epoch 4 Batch 57 Loss 2.6310\n",
      "Epoch 4 Batch 58 Loss 2.6048\n",
      "Epoch 4 Batch 59 Loss 2.5882\n",
      "14000\n",
      "Epoch 4 Loss 0.011127\n",
      "Time taken for 1 epoch 38.41669774055481 sec\n",
      "\n",
      "Epoch 5 Batch 0 Loss 2.6356\n",
      "Epoch 5 Batch 1 Loss 2.6076\n",
      "Epoch 5 Batch 2 Loss 2.6024\n",
      "Epoch 5 Batch 3 Loss 2.5595\n",
      "Epoch 5 Batch 4 Loss 2.5306\n",
      "Epoch 5 Batch 5 Loss 2.5295\n",
      "Epoch 5 Batch 6 Loss 2.5554\n",
      "Epoch 5 Batch 7 Loss 2.6119\n",
      "Epoch 5 Batch 8 Loss 2.5880\n",
      "Epoch 5 Batch 9 Loss 2.5782\n",
      "Epoch 5 Batch 10 Loss 2.5448\n",
      "Epoch 5 Batch 11 Loss 2.5105\n",
      "Epoch 5 Batch 12 Loss 2.5272\n",
      "Epoch 5 Batch 13 Loss 2.5484\n",
      "Epoch 5 Batch 14 Loss 2.6073\n",
      "Epoch 5 Batch 15 Loss 2.5869\n",
      "Epoch 5 Batch 16 Loss 2.5755\n",
      "Epoch 5 Batch 17 Loss 2.5438\n",
      "Epoch 5 Batch 18 Loss 2.5209\n",
      "Epoch 5 Batch 19 Loss 2.5261\n",
      "Epoch 5 Batch 20 Loss 2.5421\n",
      "Epoch 5 Batch 21 Loss 2.6021\n",
      "Epoch 5 Batch 22 Loss 2.5813\n",
      "Epoch 5 Batch 23 Loss 2.5566\n",
      "Epoch 5 Batch 24 Loss 2.5390\n",
      "Epoch 5 Batch 25 Loss 2.5131\n",
      "Epoch 5 Batch 26 Loss 2.5093\n",
      "Epoch 5 Batch 27 Loss 2.5261\n",
      "Epoch 5 Batch 28 Loss 2.5823\n",
      "Epoch 5 Batch 29 Loss 2.5521\n",
      "Epoch 5 Batch 30 Loss 2.5307\n",
      "Epoch 5 Batch 31 Loss 2.5315\n",
      "Epoch 5 Batch 32 Loss 2.4987\n",
      "Epoch 5 Batch 33 Loss 2.5015\n",
      "Epoch 5 Batch 34 Loss 2.5136\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5 Batch 35 Loss 2.5569\n",
      "Epoch 5 Batch 36 Loss 2.5289\n",
      "Epoch 5 Batch 37 Loss 2.5221\n",
      "Epoch 5 Batch 38 Loss 2.5172\n",
      "Epoch 5 Batch 39 Loss 2.4775\n",
      "Epoch 5 Batch 40 Loss 2.4739\n",
      "Epoch 5 Batch 41 Loss 2.4884\n",
      "Epoch 5 Batch 42 Loss 2.5219\n",
      "Epoch 5 Batch 43 Loss 2.5011\n",
      "Epoch 5 Batch 44 Loss 2.5069\n",
      "Epoch 5 Batch 45 Loss 2.4858\n",
      "Epoch 5 Batch 46 Loss 2.4410\n",
      "Epoch 5 Batch 47 Loss 2.4371\n",
      "Epoch 5 Batch 48 Loss 2.4558\n",
      "Epoch 5 Batch 49 Loss 2.4807\n",
      "Epoch 5 Batch 50 Loss 2.4904\n",
      "Epoch 5 Batch 51 Loss 2.4739\n",
      "Epoch 5 Batch 52 Loss 2.4509\n",
      "Epoch 5 Batch 53 Loss 2.4030\n",
      "Epoch 5 Batch 54 Loss 2.4022\n",
      "Epoch 5 Batch 55 Loss 2.4200\n",
      "Epoch 5 Batch 56 Loss 2.4620\n",
      "Epoch 5 Batch 57 Loss 2.4565\n",
      "Epoch 5 Batch 58 Loss 2.4328\n",
      "Epoch 5 Batch 59 Loss 2.4081\n",
      "14000\n",
      "Epoch 5 Loss 0.010798\n",
      "Time taken for 1 epoch 41.79852318763733 sec\n",
      "\n",
      "Epoch 6 Batch 0 Loss 2.4427\n",
      "Epoch 6 Batch 1 Loss 2.4159\n",
      "Epoch 6 Batch 2 Loss 2.4029\n",
      "Epoch 6 Batch 3 Loss 2.3647\n",
      "Epoch 6 Batch 4 Loss 2.3428\n",
      "Epoch 6 Batch 5 Loss 2.3415\n",
      "Epoch 6 Batch 6 Loss 2.3537\n",
      "Epoch 6 Batch 7 Loss 2.3883\n",
      "Epoch 6 Batch 8 Loss 2.3615\n",
      "Epoch 6 Batch 9 Loss 2.3489\n",
      "Epoch 6 Batch 10 Loss 2.3184\n",
      "Epoch 6 Batch 11 Loss 2.2917\n",
      "Epoch 6 Batch 12 Loss 2.2945\n",
      "Epoch 6 Batch 13 Loss 2.3056\n",
      "Epoch 6 Batch 14 Loss 2.3426\n",
      "Epoch 6 Batch 15 Loss 2.3176\n",
      "Epoch 6 Batch 16 Loss 2.3048\n",
      "Epoch 6 Batch 17 Loss 2.2738\n",
      "Epoch 6 Batch 18 Loss 2.2503\n",
      "Epoch 6 Batch 19 Loss 2.2436\n",
      "Epoch 6 Batch 20 Loss 2.2546\n",
      "Epoch 6 Batch 21 Loss 2.2925\n",
      "Epoch 6 Batch 22 Loss 2.2730\n",
      "Epoch 6 Batch 23 Loss 2.2443\n",
      "Epoch 6 Batch 24 Loss 2.2267\n",
      "Epoch 6 Batch 25 Loss 2.1930\n",
      "Epoch 6 Batch 26 Loss 2.1911\n",
      "Epoch 6 Batch 27 Loss 2.2035\n",
      "Epoch 6 Batch 28 Loss 2.2388\n",
      "Epoch 6 Batch 29 Loss 2.2054\n",
      "Epoch 6 Batch 30 Loss 2.1877\n",
      "Epoch 6 Batch 31 Loss 2.1715\n",
      "Epoch 6 Batch 32 Loss 2.1395\n",
      "Epoch 6 Batch 33 Loss 2.1391\n",
      "Epoch 6 Batch 34 Loss 2.1468\n",
      "Epoch 6 Batch 35 Loss 2.1744\n",
      "Epoch 6 Batch 36 Loss 2.1484\n",
      "Epoch 6 Batch 37 Loss 2.1422\n",
      "Epoch 6 Batch 38 Loss 2.1279\n",
      "Epoch 6 Batch 39 Loss 2.0990\n",
      "Epoch 6 Batch 40 Loss 2.0949\n",
      "Epoch 6 Batch 41 Loss 2.0956\n",
      "Epoch 6 Batch 42 Loss 2.1260\n",
      "Epoch 6 Batch 43 Loss 2.0962\n",
      "Epoch 6 Batch 44 Loss 2.0785\n",
      "Epoch 6 Batch 45 Loss 2.0646\n",
      "Epoch 6 Batch 46 Loss 2.0187\n",
      "Epoch 6 Batch 47 Loss 2.0202\n",
      "Epoch 6 Batch 48 Loss 2.0278\n",
      "Epoch 6 Batch 49 Loss 2.0467\n",
      "Epoch 6 Batch 50 Loss 2.0416\n",
      "Epoch 6 Batch 51 Loss 2.0331\n",
      "Epoch 6 Batch 52 Loss 2.0031\n",
      "Epoch 6 Batch 53 Loss 1.9650\n",
      "Epoch 6 Batch 54 Loss 1.9610\n",
      "Epoch 6 Batch 55 Loss 1.9756\n",
      "Epoch 6 Batch 56 Loss 2.0058\n",
      "Epoch 6 Batch 57 Loss 1.9964\n",
      "Epoch 6 Batch 58 Loss 1.9785\n",
      "Epoch 6 Batch 59 Loss 1.9500\n",
      "14000\n",
      "Epoch 6 Loss 0.009363\n",
      "Time taken for 1 epoch 38.852985858917236 sec\n",
      "\n",
      "Epoch 7 Batch 0 Loss 1.9886\n",
      "Epoch 7 Batch 1 Loss 1.9656\n",
      "Epoch 7 Batch 2 Loss 1.9495\n",
      "Epoch 7 Batch 3 Loss 1.9185\n",
      "Epoch 7 Batch 4 Loss 1.8982\n",
      "Epoch 7 Batch 5 Loss 1.8930\n",
      "Epoch 7 Batch 6 Loss 1.9007\n",
      "Epoch 7 Batch 7 Loss 1.9332\n",
      "Epoch 7 Batch 8 Loss 1.9071\n",
      "Epoch 7 Batch 9 Loss 1.8962\n",
      "Epoch 7 Batch 10 Loss 1.8712\n",
      "Epoch 7 Batch 11 Loss 1.8460\n",
      "Epoch 7 Batch 12 Loss 1.8497\n",
      "Epoch 7 Batch 13 Loss 1.8626\n",
      "Epoch 7 Batch 14 Loss 1.9009\n",
      "Epoch 7 Batch 15 Loss 1.8816\n",
      "Epoch 7 Batch 16 Loss 1.8665\n",
      "Epoch 7 Batch 17 Loss 1.8451\n",
      "Epoch 7 Batch 18 Loss 1.8195\n",
      "Epoch 7 Batch 19 Loss 1.8092\n",
      "Epoch 7 Batch 20 Loss 1.8200\n",
      "Epoch 7 Batch 21 Loss 1.8626\n",
      "Epoch 7 Batch 22 Loss 1.8401\n",
      "Epoch 7 Batch 23 Loss 1.8219\n",
      "Epoch 7 Batch 24 Loss 1.8065\n",
      "Epoch 7 Batch 25 Loss 1.7827\n",
      "Epoch 7 Batch 26 Loss 1.7754\n",
      "Epoch 7 Batch 27 Loss 1.7869\n",
      "Epoch 7 Batch 28 Loss 1.8249\n",
      "Epoch 7 Batch 29 Loss 1.8031\n",
      "Epoch 7 Batch 30 Loss 1.7846\n",
      "Epoch 7 Batch 31 Loss 1.7736\n",
      "Epoch 7 Batch 32 Loss 1.7399\n",
      "Epoch 7 Batch 33 Loss 1.7335\n",
      "Epoch 7 Batch 34 Loss 1.7400\n",
      "Epoch 7 Batch 35 Loss 1.7739\n",
      "Epoch 7 Batch 36 Loss 1.7534\n",
      "Epoch 7 Batch 37 Loss 1.7504\n",
      "Epoch 7 Batch 38 Loss 1.7483\n",
      "Epoch 7 Batch 39 Loss 1.7076\n",
      "Epoch 7 Batch 40 Loss 1.7027\n",
      "Epoch 7 Batch 41 Loss 1.7146\n",
      "Epoch 7 Batch 42 Loss 1.7391\n",
      "Epoch 7 Batch 43 Loss 1.7219\n",
      "Epoch 7 Batch 44 Loss 1.7142\n",
      "Epoch 7 Batch 45 Loss 1.6939\n",
      "Epoch 7 Batch 46 Loss 1.6507\n",
      "Epoch 7 Batch 47 Loss 1.6477\n",
      "Epoch 7 Batch 48 Loss 1.6565\n",
      "Epoch 7 Batch 49 Loss 1.6826\n",
      "Epoch 7 Batch 50 Loss 1.6867\n",
      "Epoch 7 Batch 51 Loss 1.6691\n",
      "Epoch 7 Batch 52 Loss 1.6507\n",
      "Epoch 7 Batch 53 Loss 1.6072\n",
      "Epoch 7 Batch 54 Loss 1.5987\n",
      "Epoch 7 Batch 55 Loss 1.6178\n",
      "Epoch 7 Batch 56 Loss 1.6611\n",
      "Epoch 7 Batch 57 Loss 1.6600\n",
      "Epoch 7 Batch 58 Loss 1.6355\n",
      "Epoch 7 Batch 59 Loss 1.6217\n",
      "14000\n",
      "Epoch 7 Loss 0.007626\n",
      "Time taken for 1 epoch 39.58919095993042 sec\n",
      "\n",
      "Epoch 8 Batch 0 Loss 1.6673\n",
      "Epoch 8 Batch 1 Loss 1.6398\n",
      "Epoch 8 Batch 2 Loss 1.6338\n",
      "Epoch 8 Batch 3 Loss 1.6029\n",
      "Epoch 8 Batch 4 Loss 1.5801\n",
      "Epoch 8 Batch 5 Loss 1.5744\n",
      "Epoch 8 Batch 6 Loss 1.5850\n",
      "Epoch 8 Batch 7 Loss 1.6216\n",
      "Epoch 8 Batch 8 Loss 1.6025\n",
      "Epoch 8 Batch 9 Loss 1.5974\n",
      "Epoch 8 Batch 10 Loss 1.5774\n",
      "Epoch 8 Batch 11 Loss 1.5453\n",
      "Epoch 8 Batch 12 Loss 1.5498\n",
      "Epoch 8 Batch 13 Loss 1.5597\n",
      "Epoch 8 Batch 14 Loss 1.6066\n",
      "Epoch 8 Batch 15 Loss 1.5930\n",
      "Epoch 8 Batch 16 Loss 1.5850\n",
      "Epoch 8 Batch 17 Loss 1.5663\n",
      "Epoch 8 Batch 18 Loss 1.5325\n",
      "Epoch 8 Batch 19 Loss 1.5199\n",
      "Epoch 8 Batch 20 Loss 1.5260\n",
      "Epoch 8 Batch 21 Loss 1.5818\n",
      "Epoch 8 Batch 22 Loss 1.5691\n",
      "Epoch 8 Batch 23 Loss 1.5529\n",
      "Epoch 8 Batch 24 Loss 1.5404\n",
      "Epoch 8 Batch 25 Loss 1.5256\n",
      "Epoch 8 Batch 26 Loss 1.5157\n",
      "Epoch 8 Batch 27 Loss 1.5280\n",
      "Epoch 8 Batch 28 Loss 1.5818\n",
      "Epoch 8 Batch 29 Loss 1.5637\n",
      "Epoch 8 Batch 30 Loss 1.5455\n",
      "Epoch 8 Batch 31 Loss 1.5470\n",
      "Epoch 8 Batch 32 Loss 1.5120\n",
      "Epoch 8 Batch 33 Loss 1.5027\n",
      "Epoch 8 Batch 34 Loss 1.5138\n",
      "Epoch 8 Batch 35 Loss 1.5607\n",
      "Epoch 8 Batch 36 Loss 1.5478\n",
      "Epoch 8 Batch 37 Loss 1.5451\n",
      "Epoch 8 Batch 38 Loss 1.5590\n",
      "Epoch 8 Batch 39 Loss 1.5100\n",
      "Epoch 8 Batch 40 Loss 1.4993\n",
      "Epoch 8 Batch 41 Loss 1.5137\n",
      "Epoch 8 Batch 42 Loss 1.5525\n",
      "Epoch 8 Batch 43 Loss 1.5441\n",
      "Epoch 8 Batch 44 Loss 1.5514\n",
      "Epoch 8 Batch 45 Loss 1.5408\n",
      "Epoch 8 Batch 46 Loss 1.4896\n",
      "Epoch 8 Batch 47 Loss 1.4791\n",
      "Epoch 8 Batch 48 Loss 1.4971\n",
      "Epoch 8 Batch 49 Loss 1.5296\n",
      "Epoch 8 Batch 50 Loss 1.5537\n",
      "Epoch 8 Batch 51 Loss 1.5382\n",
      "Epoch 8 Batch 52 Loss 1.5288\n",
      "Epoch 8 Batch 53 Loss 1.4758\n",
      "Epoch 8 Batch 54 Loss 1.4615\n",
      "Epoch 8 Batch 55 Loss 1.4878\n",
      "Epoch 8 Batch 56 Loss 1.5445\n",
      "Epoch 8 Batch 57 Loss 1.5587\n",
      "Epoch 8 Batch 58 Loss 1.5356\n",
      "Epoch 8 Batch 59 Loss 1.5262\n",
      "14000\n",
      "Epoch 8 Loss 0.006641\n",
      "Time taken for 1 epoch 37.995529890060425 sec\n",
      "\n",
      "Epoch 9 Batch 0 Loss 1.5650\n",
      "Epoch 9 Batch 1 Loss 1.5435\n",
      "Epoch 9 Batch 2 Loss 1.5442\n",
      "Epoch 9 Batch 3 Loss 1.5123\n",
      "Epoch 9 Batch 4 Loss 1.4762\n",
      "Epoch 9 Batch 5 Loss 1.4735\n",
      "Epoch 9 Batch 6 Loss 1.4859\n",
      "Epoch 9 Batch 7 Loss 1.5372\n",
      "Epoch 9 Batch 8 Loss 1.5211\n",
      "Epoch 9 Batch 9 Loss 1.5194\n",
      "Epoch 9 Batch 10 Loss 1.5003\n",
      "Epoch 9 Batch 11 Loss 1.4589\n",
      "Epoch 9 Batch 12 Loss 1.4711\n",
      "Epoch 9 Batch 13 Loss 1.4865\n",
      "Epoch 9 Batch 14 Loss 1.5497\n",
      "Epoch 9 Batch 15 Loss 1.5373\n",
      "Epoch 9 Batch 16 Loss 1.5320\n",
      "Epoch 9 Batch 17 Loss 1.5094\n",
      "Epoch 9 Batch 18 Loss 1.4687\n",
      "Epoch 9 Batch 19 Loss 1.4608\n",
      "Epoch 9 Batch 20 Loss 1.4668\n",
      "Epoch 9 Batch 21 Loss 1.5304\n",
      "Epoch 9 Batch 22 Loss 1.5211\n",
      "Epoch 9 Batch 23 Loss 1.5035\n",
      "Epoch 9 Batch 24 Loss 1.4921\n",
      "Epoch 9 Batch 25 Loss 1.4741\n",
      "Epoch 9 Batch 26 Loss 1.4673\n",
      "Epoch 9 Batch 27 Loss 1.4765\n",
      "Epoch 9 Batch 28 Loss 1.5367\n",
      "Epoch 9 Batch 29 Loss 1.5213\n",
      "Epoch 9 Batch 30 Loss 1.5030\n",
      "Epoch 9 Batch 31 Loss 1.5080\n",
      "Epoch 9 Batch 32 Loss 1.4697\n",
      "Epoch 9 Batch 33 Loss 1.4607\n",
      "Epoch 9 Batch 34 Loss 1.4724\n",
      "Epoch 9 Batch 35 Loss 1.5227\n",
      "Epoch 9 Batch 36 Loss 1.5112\n",
      "Epoch 9 Batch 37 Loss 1.5103\n",
      "Epoch 9 Batch 38 Loss 1.5261\n",
      "Epoch 9 Batch 39 Loss 1.4740\n",
      "Epoch 9 Batch 40 Loss 1.4638\n",
      "Epoch 9 Batch 41 Loss 1.4772\n",
      "Epoch 9 Batch 42 Loss 1.5199\n",
      "Epoch 9 Batch 43 Loss 1.5145\n",
      "Epoch 9 Batch 44 Loss 1.5262\n",
      "Epoch 9 Batch 45 Loss 1.5163\n",
      "Epoch 9 Batch 46 Loss 1.4620\n",
      "Epoch 9 Batch 47 Loss 1.4474\n",
      "Epoch 9 Batch 48 Loss 1.4681\n",
      "Epoch 9 Batch 49 Loss 1.5023\n",
      "Epoch 9 Batch 50 Loss 1.5342\n",
      "Epoch 9 Batch 51 Loss 1.5178\n",
      "Epoch 9 Batch 52 Loss 1.5092\n",
      "Epoch 9 Batch 53 Loss 1.4507\n",
      "Epoch 9 Batch 54 Loss 1.4361\n",
      "Epoch 9 Batch 55 Loss 1.4636\n",
      "Epoch 9 Batch 56 Loss 1.5304\n",
      "Epoch 9 Batch 57 Loss 1.5453\n",
      "Epoch 9 Batch 58 Loss 1.5213\n",
      "Epoch 9 Batch 59 Loss 1.5141\n",
      "14000\n",
      "Epoch 9 Loss 0.006430\n",
      "Time taken for 1 epoch 40.4141309261322 sec\n",
      "\n",
      "Epoch 10 Batch 0 Loss 1.5494\n",
      "Epoch 10 Batch 1 Loss 1.5254\n",
      "Epoch 10 Batch 2 Loss 1.5277\n",
      "Epoch 10 Batch 3 Loss 1.4941\n",
      "Epoch 10 Batch 4 Loss 1.4548\n",
      "Epoch 10 Batch 5 Loss 1.4556\n",
      "Epoch 10 Batch 6 Loss 1.4689\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10 Batch 7 Loss 1.5277\n",
      "Epoch 10 Batch 8 Loss 1.5096\n",
      "Epoch 10 Batch 9 Loss 1.5064\n",
      "Epoch 10 Batch 10 Loss 1.4836\n",
      "Epoch 10 Batch 11 Loss 1.4442\n",
      "Epoch 10 Batch 12 Loss 1.4556\n",
      "Epoch 10 Batch 13 Loss 1.4666\n",
      "Epoch 10 Batch 14 Loss 1.5298\n",
      "Epoch 10 Batch 15 Loss 1.5175\n",
      "Epoch 10 Batch 16 Loss 1.5122\n",
      "Epoch 10 Batch 17 Loss 1.4882\n",
      "Epoch 10 Batch 18 Loss 1.4485\n",
      "Epoch 10 Batch 19 Loss 1.4528\n",
      "Epoch 10 Batch 20 Loss 1.4586\n",
      "Epoch 10 Batch 21 Loss 1.5213\n",
      "Epoch 10 Batch 22 Loss 1.5109\n",
      "Epoch 10 Batch 23 Loss 1.4944\n",
      "Epoch 10 Batch 24 Loss 1.4801\n",
      "Epoch 10 Batch 25 Loss 1.4582\n",
      "Epoch 10 Batch 26 Loss 1.4509\n",
      "Epoch 10 Batch 27 Loss 1.4657\n",
      "Epoch 10 Batch 28 Loss 1.5247\n",
      "Epoch 10 Batch 29 Loss 1.5110\n",
      "Epoch 10 Batch 30 Loss 1.4911\n",
      "Epoch 10 Batch 31 Loss 1.4942\n",
      "Epoch 10 Batch 32 Loss 1.4604\n",
      "Epoch 10 Batch 33 Loss 1.4506\n",
      "Epoch 10 Batch 34 Loss 1.4643\n",
      "Epoch 10 Batch 35 Loss 1.5139\n",
      "Epoch 10 Batch 36 Loss 1.5025\n",
      "Epoch 10 Batch 37 Loss 1.5005\n",
      "Epoch 10 Batch 38 Loss 1.5103\n",
      "Epoch 10 Batch 39 Loss 1.4598\n",
      "Epoch 10 Batch 40 Loss 1.4544\n",
      "Epoch 10 Batch 41 Loss 1.4701\n",
      "Epoch 10 Batch 42 Loss 1.5089\n",
      "Epoch 10 Batch 43 Loss 1.5024\n",
      "Epoch 10 Batch 44 Loss 1.5121\n",
      "Epoch 10 Batch 45 Loss 1.4996\n",
      "Epoch 10 Batch 46 Loss 1.4494\n",
      "Epoch 10 Batch 47 Loss 1.4406\n",
      "Epoch 10 Batch 48 Loss 1.4583\n",
      "Epoch 10 Batch 49 Loss 1.4907\n",
      "Epoch 10 Batch 50 Loss 1.5198\n",
      "Epoch 10 Batch 51 Loss 1.5036\n",
      "Epoch 10 Batch 52 Loss 1.4944\n",
      "Epoch 10 Batch 53 Loss 1.4417\n",
      "Epoch 10 Batch 54 Loss 1.4290\n",
      "Epoch 10 Batch 55 Loss 1.4540\n",
      "Epoch 10 Batch 56 Loss 1.5213\n",
      "Epoch 10 Batch 57 Loss 1.5325\n",
      "Epoch 10 Batch 58 Loss 1.5097\n",
      "Epoch 10 Batch 59 Loss 1.5036\n",
      "14000\n",
      "Epoch 10 Loss 0.006374\n",
      "Time taken for 1 epoch 41.60913896560669 sec\n",
      "\n",
      "Epoch 11 Batch 0 Loss 1.5351\n",
      "Epoch 11 Batch 1 Loss 1.5098\n",
      "Epoch 11 Batch 2 Loss 1.5134\n",
      "Epoch 11 Batch 3 Loss 1.4854\n",
      "Epoch 11 Batch 4 Loss 1.4513\n",
      "Epoch 11 Batch 5 Loss 1.4476\n",
      "Epoch 11 Batch 6 Loss 1.4633\n",
      "Epoch 11 Batch 7 Loss 1.5154\n",
      "Epoch 11 Batch 8 Loss 1.5014\n",
      "Epoch 11 Batch 9 Loss 1.4992\n",
      "Epoch 11 Batch 10 Loss 1.4768\n",
      "Epoch 11 Batch 11 Loss 1.4346\n",
      "Epoch 11 Batch 12 Loss 1.4528\n",
      "Epoch 11 Batch 13 Loss 1.4679\n",
      "Epoch 11 Batch 14 Loss 1.5224\n",
      "Epoch 11 Batch 15 Loss 1.5102\n",
      "Epoch 11 Batch 16 Loss 1.5077\n",
      "Epoch 11 Batch 17 Loss 1.4847\n",
      "Epoch 11 Batch 18 Loss 1.4451\n",
      "Epoch 11 Batch 19 Loss 1.4380\n",
      "Epoch 11 Batch 20 Loss 1.4502\n",
      "Epoch 11 Batch 21 Loss 1.5117\n",
      "Epoch 11 Batch 22 Loss 1.4988\n",
      "Epoch 11 Batch 23 Loss 1.4842\n",
      "Epoch 11 Batch 24 Loss 1.4637\n",
      "Epoch 11 Batch 25 Loss 1.4483\n",
      "Epoch 11 Batch 26 Loss 1.4488\n",
      "Epoch 11 Batch 27 Loss 1.4612\n",
      "Epoch 11 Batch 28 Loss 1.5152\n",
      "Epoch 11 Batch 29 Loss 1.4996\n",
      "Epoch 11 Batch 30 Loss 1.4781\n",
      "Epoch 11 Batch 31 Loss 1.4838\n",
      "Epoch 11 Batch 32 Loss 1.4579\n",
      "Epoch 11 Batch 33 Loss 1.4514\n",
      "Epoch 11 Batch 34 Loss 1.4583\n",
      "Epoch 11 Batch 35 Loss 1.5068\n",
      "Epoch 11 Batch 36 Loss 1.4933\n",
      "Epoch 11 Batch 37 Loss 1.4917\n",
      "Epoch 11 Batch 38 Loss 1.5080\n",
      "Epoch 11 Batch 39 Loss 1.4636\n",
      "Epoch 11 Batch 40 Loss 1.4499\n",
      "Epoch 11 Batch 41 Loss 1.4646\n",
      "Epoch 11 Batch 42 Loss 1.5019\n",
      "Epoch 11 Batch 43 Loss 1.4963\n",
      "Epoch 11 Batch 44 Loss 1.5105\n",
      "Epoch 11 Batch 45 Loss 1.5033\n",
      "Epoch 11 Batch 46 Loss 1.4493\n",
      "Epoch 11 Batch 47 Loss 1.4357\n",
      "Epoch 11 Batch 48 Loss 1.4527\n",
      "Epoch 11 Batch 49 Loss 1.4863\n",
      "Epoch 11 Batch 50 Loss 1.5208\n",
      "Epoch 11 Batch 51 Loss 1.5080\n",
      "Epoch 11 Batch 52 Loss 1.4980\n",
      "Epoch 11 Batch 53 Loss 1.4397\n",
      "Epoch 11 Batch 54 Loss 1.4241\n",
      "Epoch 11 Batch 55 Loss 1.4511\n",
      "Epoch 11 Batch 56 Loss 1.5250\n",
      "Epoch 11 Batch 57 Loss 1.5371\n",
      "Epoch 11 Batch 58 Loss 1.5129\n",
      "Epoch 11 Batch 59 Loss 1.5050\n",
      "14000\n",
      "Epoch 11 Loss 0.006351\n",
      "Time taken for 1 epoch 38.157991886138916 sec\n",
      "\n",
      "Epoch 12 Batch 0 Loss 1.5360\n",
      "Epoch 12 Batch 1 Loss 1.5065\n",
      "Epoch 12 Batch 2 Loss 1.5076\n",
      "Epoch 12 Batch 3 Loss 1.4743\n",
      "Epoch 12 Batch 4 Loss 1.4391\n",
      "Epoch 12 Batch 5 Loss 1.4380\n",
      "Epoch 12 Batch 6 Loss 1.4551\n",
      "Epoch 12 Batch 7 Loss 1.5114\n",
      "Epoch 12 Batch 8 Loss 1.4957\n",
      "Epoch 12 Batch 9 Loss 1.4935\n",
      "Epoch 12 Batch 10 Loss 1.4687\n",
      "Epoch 12 Batch 11 Loss 1.4242\n",
      "Epoch 12 Batch 12 Loss 1.4433\n",
      "Epoch 12 Batch 13 Loss 1.4600\n",
      "Epoch 12 Batch 14 Loss 1.5167\n",
      "Epoch 12 Batch 15 Loss 1.5036\n",
      "Epoch 12 Batch 16 Loss 1.5007\n",
      "Epoch 12 Batch 17 Loss 1.4805\n",
      "Epoch 12 Batch 18 Loss 1.4452\n",
      "Epoch 12 Batch 19 Loss 1.4388\n",
      "Epoch 12 Batch 20 Loss 1.4453\n",
      "Epoch 12 Batch 21 Loss 1.5067\n",
      "Epoch 12 Batch 22 Loss 1.4979\n",
      "Epoch 12 Batch 23 Loss 1.4804\n",
      "Epoch 12 Batch 24 Loss 1.4590\n",
      "Epoch 12 Batch 25 Loss 1.4436\n",
      "Epoch 12 Batch 26 Loss 1.4438\n",
      "Epoch 12 Batch 27 Loss 1.4568\n",
      "Epoch 12 Batch 28 Loss 1.5140\n",
      "Epoch 12 Batch 29 Loss 1.4948\n",
      "Epoch 12 Batch 30 Loss 1.4744\n",
      "Epoch 12 Batch 31 Loss 1.4772\n",
      "Epoch 12 Batch 32 Loss 1.4499\n",
      "Epoch 12 Batch 33 Loss 1.4433\n",
      "Epoch 12 Batch 34 Loss 1.4591\n",
      "Epoch 12 Batch 35 Loss 1.5038\n",
      "Epoch 12 Batch 36 Loss 1.4881\n",
      "Epoch 12 Batch 37 Loss 1.4878\n",
      "Epoch 12 Batch 38 Loss 1.4977\n",
      "Epoch 12 Batch 39 Loss 1.4568\n",
      "Epoch 12 Batch 40 Loss 1.4504\n",
      "Epoch 12 Batch 41 Loss 1.4673\n",
      "Epoch 12 Batch 42 Loss 1.4998\n",
      "Epoch 12 Batch 43 Loss 1.4897\n",
      "Epoch 12 Batch 44 Loss 1.5029\n",
      "Epoch 12 Batch 45 Loss 1.4916\n",
      "Epoch 12 Batch 46 Loss 1.4481\n",
      "Epoch 12 Batch 47 Loss 1.4410\n",
      "Epoch 12 Batch 48 Loss 1.4563\n",
      "Epoch 12 Batch 49 Loss 1.4822\n",
      "Epoch 12 Batch 50 Loss 1.5091\n",
      "Epoch 12 Batch 51 Loss 1.4959\n",
      "Epoch 12 Batch 52 Loss 1.4846\n",
      "Epoch 12 Batch 53 Loss 1.4358\n",
      "Epoch 12 Batch 54 Loss 1.4252\n",
      "Epoch 12 Batch 55 Loss 1.4456\n",
      "Epoch 12 Batch 56 Loss 1.5131\n",
      "Epoch 12 Batch 57 Loss 1.5254\n",
      "Epoch 12 Batch 58 Loss 1.5044\n",
      "Epoch 12 Batch 59 Loss 1.4929\n",
      "14000\n",
      "Epoch 12 Loss 0.006327\n",
      "Time taken for 1 epoch 38.84334588050842 sec\n",
      "\n",
      "Epoch 13 Batch 0 Loss 1.5221\n",
      "Epoch 13 Batch 1 Loss 1.4986\n",
      "Epoch 13 Batch 2 Loss 1.4956\n",
      "Epoch 13 Batch 3 Loss 1.4654\n",
      "Epoch 13 Batch 4 Loss 1.4366\n",
      "Epoch 13 Batch 5 Loss 1.4312\n",
      "Epoch 13 Batch 6 Loss 1.4510\n",
      "Epoch 13 Batch 7 Loss 1.4995\n",
      "Epoch 13 Batch 8 Loss 1.4831\n",
      "Epoch 13 Batch 9 Loss 1.4792\n",
      "Epoch 13 Batch 10 Loss 1.4634\n",
      "Epoch 13 Batch 11 Loss 1.4305\n",
      "Epoch 13 Batch 12 Loss 1.4476\n",
      "Epoch 13 Batch 13 Loss 1.4669\n",
      "Epoch 13 Batch 14 Loss 1.5079\n",
      "Epoch 13 Batch 15 Loss 1.4943\n",
      "Epoch 13 Batch 16 Loss 1.4945\n",
      "Epoch 13 Batch 17 Loss 1.4810\n",
      "Epoch 13 Batch 18 Loss 1.4523\n",
      "Epoch 13 Batch 19 Loss 1.4449\n",
      "Epoch 13 Batch 20 Loss 1.4476\n",
      "Epoch 13 Batch 21 Loss 1.4932\n",
      "Epoch 13 Batch 22 Loss 1.4835\n",
      "Epoch 13 Batch 23 Loss 1.4731\n",
      "Epoch 13 Batch 24 Loss 1.4749\n",
      "Epoch 13 Batch 25 Loss 1.4444\n",
      "Epoch 13 Batch 26 Loss 1.4463\n",
      "Epoch 13 Batch 27 Loss 1.5125\n",
      "Epoch 13 Batch 28 Loss 1.5138\n",
      "Epoch 13 Batch 29 Loss 1.4876\n",
      "Epoch 13 Batch 30 Loss 1.4760\n",
      "Epoch 13 Batch 31 Loss 1.5234\n",
      "Epoch 13 Batch 32 Loss 1.4455\n",
      "Epoch 13 Batch 33 Loss 1.4364\n",
      "Epoch 13 Batch 34 Loss 1.4474\n",
      "Epoch 13 Batch 35 Loss 1.4975\n",
      "Epoch 13 Batch 36 Loss 1.4786\n",
      "Epoch 13 Batch 37 Loss 1.4719\n",
      "Epoch 13 Batch 38 Loss 1.4799\n",
      "Epoch 13 Batch 39 Loss 1.4599\n",
      "Epoch 13 Batch 40 Loss 1.4662\n",
      "Epoch 13 Batch 41 Loss 1.4754\n",
      "Epoch 13 Batch 42 Loss 1.4980\n",
      "Epoch 13 Batch 43 Loss 1.4797\n",
      "Epoch 13 Batch 44 Loss 1.4973\n",
      "Epoch 13 Batch 45 Loss 1.4878\n",
      "Epoch 13 Batch 46 Loss 1.4489\n",
      "Epoch 13 Batch 47 Loss 1.4445\n",
      "Epoch 13 Batch 48 Loss 1.4601\n",
      "Epoch 13 Batch 49 Loss 1.4805\n",
      "Epoch 13 Batch 50 Loss 1.5154\n",
      "Epoch 13 Batch 51 Loss 1.5029\n",
      "Epoch 13 Batch 52 Loss 1.4941\n",
      "Epoch 13 Batch 53 Loss 1.4503\n",
      "Epoch 13 Batch 54 Loss 1.4399\n",
      "Epoch 13 Batch 55 Loss 1.4542\n",
      "Epoch 13 Batch 56 Loss 1.5214\n",
      "Epoch 13 Batch 57 Loss 1.5343\n",
      "Epoch 13 Batch 58 Loss 1.5159\n",
      "Epoch 13 Batch 59 Loss 1.5088\n",
      "14000\n",
      "Epoch 13 Loss 0.006330\n",
      "Time taken for 1 epoch 41.29414701461792 sec\n",
      "\n",
      "Epoch 14 Batch 0 Loss 1.5293\n",
      "Epoch 14 Batch 1 Loss 1.5090\n",
      "Epoch 14 Batch 2 Loss 1.5095\n",
      "Epoch 14 Batch 3 Loss 1.4816\n",
      "Epoch 14 Batch 4 Loss 1.4445\n",
      "Epoch 14 Batch 5 Loss 1.4336\n",
      "Epoch 14 Batch 6 Loss 1.4454\n",
      "Epoch 14 Batch 7 Loss 1.5010\n",
      "Epoch 14 Batch 8 Loss 1.4872\n",
      "Epoch 14 Batch 9 Loss 1.4861\n",
      "Epoch 14 Batch 10 Loss 1.4645\n",
      "Epoch 14 Batch 11 Loss 1.4208\n",
      "Epoch 14 Batch 12 Loss 1.4289\n",
      "Epoch 14 Batch 13 Loss 1.4449\n",
      "Epoch 14 Batch 14 Loss 1.5076\n",
      "Epoch 14 Batch 15 Loss 1.4972\n",
      "Epoch 14 Batch 16 Loss 1.4924\n",
      "Epoch 14 Batch 17 Loss 1.4718\n",
      "Epoch 14 Batch 18 Loss 1.4387\n",
      "Epoch 14 Batch 19 Loss 1.4402\n",
      "Epoch 14 Batch 20 Loss 1.4531\n",
      "Epoch 14 Batch 21 Loss 1.5120\n",
      "Epoch 14 Batch 22 Loss 1.5031\n",
      "Epoch 14 Batch 23 Loss 1.4887\n",
      "Epoch 14 Batch 24 Loss 1.4757\n",
      "Epoch 14 Batch 25 Loss 1.4480\n",
      "Epoch 14 Batch 26 Loss 1.4423\n",
      "Epoch 14 Batch 27 Loss 1.4600\n",
      "Epoch 14 Batch 28 Loss 1.5088\n",
      "Epoch 14 Batch 29 Loss 1.4971\n",
      "Epoch 14 Batch 30 Loss 1.4807\n",
      "Epoch 14 Batch 31 Loss 1.4777\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14 Batch 32 Loss 1.4407\n",
      "Epoch 14 Batch 33 Loss 1.4379\n",
      "Epoch 14 Batch 34 Loss 1.4512\n",
      "Epoch 14 Batch 35 Loss 1.4907\n",
      "Epoch 14 Batch 36 Loss 1.4807\n",
      "Epoch 14 Batch 37 Loss 1.4761\n",
      "Epoch 14 Batch 38 Loss 1.4817\n",
      "Epoch 14 Batch 39 Loss 1.4384\n",
      "Epoch 14 Batch 40 Loss 1.4392\n",
      "Epoch 14 Batch 41 Loss 1.4516\n",
      "Epoch 14 Batch 42 Loss 1.4858\n",
      "Epoch 14 Batch 43 Loss 1.4770\n",
      "Epoch 14 Batch 44 Loss 1.4880\n",
      "Epoch 14 Batch 45 Loss 1.4725\n",
      "Epoch 14 Batch 46 Loss 1.4343\n",
      "Epoch 14 Batch 47 Loss 1.4319\n",
      "Epoch 14 Batch 48 Loss 1.4465\n",
      "Epoch 14 Batch 49 Loss 1.4694\n",
      "Epoch 14 Batch 50 Loss 1.5014\n",
      "Epoch 14 Batch 51 Loss 1.4876\n",
      "Epoch 14 Batch 52 Loss 1.4776\n",
      "Epoch 14 Batch 53 Loss 1.4434\n",
      "Epoch 14 Batch 54 Loss 1.4342\n",
      "Epoch 14 Batch 55 Loss 1.4456\n",
      "Epoch 14 Batch 56 Loss 1.5095\n",
      "Epoch 14 Batch 57 Loss 1.5210\n",
      "Epoch 14 Batch 58 Loss 1.5044\n",
      "Epoch 14 Batch 59 Loss 1.5039\n",
      "14000\n",
      "Epoch 14 Loss 0.006307\n",
      "Time taken for 1 epoch 39.94004201889038 sec\n",
      "\n",
      "Epoch 15 Batch 0 Loss 1.5153\n",
      "Epoch 15 Batch 1 Loss 1.5011\n",
      "Epoch 15 Batch 2 Loss 1.5079\n",
      "Epoch 15 Batch 3 Loss 1.4950\n",
      "Epoch 15 Batch 4 Loss 1.4548\n",
      "Epoch 15 Batch 5 Loss 1.4355\n",
      "Epoch 15 Batch 6 Loss 1.4427\n",
      "Epoch 15 Batch 7 Loss 1.4978\n",
      "Epoch 15 Batch 8 Loss 1.4899\n",
      "Epoch 15 Batch 9 Loss 1.4933\n",
      "Epoch 15 Batch 10 Loss 1.4756\n",
      "Epoch 15 Batch 11 Loss 1.4257\n",
      "Epoch 15 Batch 12 Loss 1.4269\n",
      "Epoch 15 Batch 13 Loss 1.4393\n",
      "Epoch 15 Batch 14 Loss 1.5033\n",
      "Epoch 15 Batch 15 Loss 1.4921\n",
      "Epoch 15 Batch 16 Loss 1.4858\n",
      "Epoch 15 Batch 17 Loss 1.4676\n",
      "Epoch 15 Batch 18 Loss 1.4313\n",
      "Epoch 15 Batch 19 Loss 1.4331\n",
      "Epoch 15 Batch 20 Loss 1.4436\n",
      "Epoch 15 Batch 21 Loss 1.5032\n",
      "Epoch 15 Batch 22 Loss 1.4932\n",
      "Epoch 15 Batch 23 Loss 1.4804\n",
      "Epoch 15 Batch 24 Loss 1.4668\n",
      "Epoch 15 Batch 25 Loss 1.4536\n",
      "Epoch 15 Batch 26 Loss 1.4423\n",
      "Epoch 15 Batch 27 Loss 1.4564\n",
      "Epoch 15 Batch 28 Loss 1.5049\n",
      "Epoch 15 Batch 29 Loss 1.4961\n",
      "Epoch 15 Batch 30 Loss 1.4768\n",
      "Epoch 15 Batch 31 Loss 1.4936\n",
      "Epoch 15 Batch 32 Loss 1.4597\n",
      "Epoch 15 Batch 33 Loss 1.4472\n",
      "Epoch 15 Batch 34 Loss 1.4560\n",
      "Epoch 15 Batch 35 Loss 1.4972\n",
      "Epoch 15 Batch 36 Loss 1.4922\n",
      "Epoch 15 Batch 37 Loss 1.4897\n",
      "Epoch 15 Batch 38 Loss 1.5041\n",
      "Epoch 15 Batch 39 Loss 1.4507\n",
      "Epoch 15 Batch 40 Loss 1.4427\n",
      "Epoch 15 Batch 41 Loss 1.4601\n",
      "Epoch 15 Batch 42 Loss 1.4927\n",
      "Epoch 15 Batch 43 Loss 1.4867\n",
      "Epoch 15 Batch 44 Loss 1.4991\n",
      "Epoch 15 Batch 45 Loss 1.4836\n",
      "Epoch 15 Batch 46 Loss 1.4311\n",
      "Epoch 15 Batch 47 Loss 1.4309\n",
      "Epoch 15 Batch 48 Loss 1.4482\n",
      "Epoch 15 Batch 49 Loss 1.4758\n",
      "Epoch 15 Batch 50 Loss 1.5047\n",
      "Epoch 15 Batch 51 Loss 1.4917\n",
      "Epoch 15 Batch 52 Loss 1.4730\n",
      "Epoch 15 Batch 53 Loss 1.4354\n",
      "Epoch 15 Batch 54 Loss 1.4327\n",
      "Epoch 15 Batch 55 Loss 1.4512\n",
      "Epoch 15 Batch 56 Loss 1.5176\n",
      "Epoch 15 Batch 57 Loss 1.5265\n",
      "Epoch 15 Batch 58 Loss 1.5090\n",
      "Epoch 15 Batch 59 Loss 1.5000\n",
      "14000\n",
      "Epoch 15 Loss 0.006315\n",
      "Time taken for 1 epoch 39.83094501495361 sec\n",
      "\n",
      "Epoch 16 Batch 0 Loss 1.5169\n",
      "Epoch 16 Batch 1 Loss 1.5013\n",
      "Epoch 16 Batch 2 Loss 1.5021\n",
      "Epoch 16 Batch 3 Loss 1.4914\n",
      "Epoch 16 Batch 4 Loss 1.4724\n",
      "Epoch 16 Batch 5 Loss 1.4586\n",
      "Epoch 16 Batch 6 Loss 1.4667\n",
      "Epoch 16 Batch 7 Loss 1.5029\n",
      "Epoch 16 Batch 8 Loss 1.4890\n",
      "Epoch 16 Batch 9 Loss 1.4872\n",
      "Epoch 16 Batch 10 Loss 1.4693\n",
      "Epoch 16 Batch 11 Loss 1.4435\n",
      "Epoch 16 Batch 12 Loss 1.4458\n",
      "Epoch 16 Batch 13 Loss 1.4574\n",
      "Epoch 16 Batch 14 Loss 1.5011\n",
      "Epoch 16 Batch 15 Loss 1.4892\n",
      "Epoch 16 Batch 16 Loss 1.4822\n",
      "Epoch 16 Batch 17 Loss 1.4642\n",
      "Epoch 16 Batch 18 Loss 1.4373\n",
      "Epoch 16 Batch 19 Loss 1.4362\n",
      "Epoch 16 Batch 20 Loss 1.4477\n",
      "Epoch 16 Batch 21 Loss 1.4971\n",
      "Epoch 16 Batch 22 Loss 1.4863\n",
      "Epoch 16 Batch 23 Loss 1.4718\n",
      "Epoch 16 Batch 24 Loss 1.4560\n",
      "Epoch 16 Batch 25 Loss 1.4435\n",
      "Epoch 16 Batch 26 Loss 1.4409\n",
      "Epoch 16 Batch 27 Loss 1.4568\n",
      "Epoch 16 Batch 28 Loss 1.5045\n",
      "Epoch 16 Batch 29 Loss 1.4862\n",
      "Epoch 16 Batch 30 Loss 1.4680\n",
      "Epoch 16 Batch 31 Loss 1.4731\n",
      "Epoch 16 Batch 32 Loss 1.4501\n",
      "Epoch 16 Batch 33 Loss 1.4481\n",
      "Epoch 16 Batch 34 Loss 1.4607\n",
      "Epoch 16 Batch 35 Loss 1.5011\n",
      "Epoch 16 Batch 36 Loss 1.4826\n",
      "Epoch 16 Batch 37 Loss 1.4871\n",
      "Epoch 16 Batch 38 Loss 1.5003\n",
      "Epoch 16 Batch 39 Loss 1.4649\n",
      "Epoch 16 Batch 40 Loss 1.4594\n",
      "Epoch 16 Batch 41 Loss 1.4756\n",
      "Epoch 16 Batch 42 Loss 1.5036\n",
      "Epoch 16 Batch 43 Loss 1.4889\n",
      "Epoch 16 Batch 44 Loss 1.4974\n",
      "Epoch 16 Batch 45 Loss 1.4856\n",
      "Epoch 16 Batch 46 Loss 1.4438\n",
      "Epoch 16 Batch 47 Loss 1.4393\n",
      "Epoch 16 Batch 48 Loss 1.4544\n",
      "Epoch 16 Batch 49 Loss 1.4772\n",
      "Epoch 16 Batch 50 Loss 1.5034\n",
      "Epoch 16 Batch 51 Loss 1.4931\n",
      "Epoch 16 Batch 52 Loss 1.4793\n",
      "Epoch 16 Batch 53 Loss 1.4351\n",
      "Epoch 16 Batch 54 Loss 1.4297\n",
      "Epoch 16 Batch 55 Loss 1.4504\n",
      "Epoch 16 Batch 56 Loss 1.5234\n",
      "Epoch 16 Batch 57 Loss 1.5372\n",
      "Epoch 16 Batch 58 Loss 1.5167\n",
      "Epoch 16 Batch 59 Loss 1.5035\n",
      "14000\n",
      "Epoch 16 Loss 0.006324\n",
      "Time taken for 1 epoch 39.42359232902527 sec\n",
      "\n",
      "Epoch 17 Batch 0 Loss 1.5234\n",
      "Epoch 17 Batch 1 Loss 1.5004\n",
      "Epoch 17 Batch 2 Loss 1.4958\n",
      "Epoch 17 Batch 3 Loss 1.4643\n",
      "Epoch 17 Batch 4 Loss 1.4475\n",
      "Epoch 17 Batch 5 Loss 1.4465\n",
      "Epoch 17 Batch 6 Loss 1.4666\n",
      "Epoch 17 Batch 7 Loss 1.5077\n",
      "Epoch 17 Batch 8 Loss 1.4894\n",
      "Epoch 17 Batch 9 Loss 1.4854\n",
      "Epoch 17 Batch 10 Loss 1.4662\n",
      "Epoch 17 Batch 11 Loss 1.4418\n",
      "Epoch 17 Batch 12 Loss 1.4478\n",
      "Epoch 17 Batch 13 Loss 1.4639\n",
      "Epoch 17 Batch 14 Loss 1.5031\n",
      "Epoch 17 Batch 15 Loss 1.4915\n",
      "Epoch 17 Batch 16 Loss 1.4874\n",
      "Epoch 17 Batch 17 Loss 1.4695\n",
      "Epoch 17 Batch 18 Loss 1.4403\n",
      "Epoch 17 Batch 19 Loss 1.4414\n",
      "Epoch 17 Batch 20 Loss 1.4521\n",
      "Epoch 17 Batch 21 Loss 1.4981\n",
      "Epoch 17 Batch 22 Loss 1.4879\n",
      "Epoch 17 Batch 23 Loss 1.4759\n",
      "Epoch 17 Batch 24 Loss 1.4566\n",
      "Epoch 17 Batch 25 Loss 1.4426\n",
      "Epoch 17 Batch 26 Loss 1.4430\n",
      "Epoch 17 Batch 27 Loss 1.4569\n",
      "Epoch 17 Batch 28 Loss 1.5036\n",
      "Epoch 17 Batch 29 Loss 1.4862\n",
      "Epoch 17 Batch 30 Loss 1.4673\n",
      "Epoch 17 Batch 31 Loss 1.4618\n",
      "Epoch 17 Batch 32 Loss 1.4397\n",
      "Epoch 17 Batch 33 Loss 1.4414\n",
      "Epoch 17 Batch 34 Loss 1.4555\n",
      "Epoch 17 Batch 35 Loss 1.4958\n",
      "Epoch 17 Batch 36 Loss 1.4753\n",
      "Epoch 17 Batch 37 Loss 1.4743\n",
      "Epoch 17 Batch 38 Loss 1.4789\n",
      "Epoch 17 Batch 39 Loss 1.4513\n",
      "Epoch 17 Batch 40 Loss 1.4509\n",
      "Epoch 17 Batch 41 Loss 1.4684\n",
      "Epoch 17 Batch 42 Loss 1.4963\n",
      "Epoch 17 Batch 43 Loss 1.4770\n",
      "Epoch 17 Batch 44 Loss 1.4843\n",
      "Epoch 17 Batch 45 Loss 1.4716\n",
      "Epoch 17 Batch 46 Loss 1.4402\n",
      "Epoch 17 Batch 47 Loss 1.4397\n",
      "Epoch 17 Batch 48 Loss 1.4549\n",
      "Epoch 17 Batch 49 Loss 1.4743\n",
      "Epoch 17 Batch 50 Loss 1.4942\n",
      "Epoch 17 Batch 51 Loss 1.4843\n",
      "Epoch 17 Batch 52 Loss 1.4730\n",
      "Epoch 17 Batch 53 Loss 1.4375\n",
      "Epoch 17 Batch 54 Loss 1.4335\n",
      "Epoch 17 Batch 55 Loss 1.4512\n",
      "Epoch 17 Batch 56 Loss 1.5191\n",
      "Epoch 17 Batch 57 Loss 1.5297\n",
      "Epoch 17 Batch 58 Loss 1.5124\n",
      "Epoch 17 Batch 59 Loss 1.5013\n",
      "14000\n",
      "Epoch 17 Loss 0.006308\n",
      "Time taken for 1 epoch 40.002460956573486 sec\n",
      "\n",
      "Epoch 18 Batch 0 Loss 1.5112\n",
      "Epoch 18 Batch 1 Loss 1.4912\n",
      "Epoch 18 Batch 2 Loss 1.4879\n",
      "Epoch 18 Batch 3 Loss 1.4611\n",
      "Epoch 18 Batch 4 Loss 1.4442\n",
      "Epoch 18 Batch 5 Loss 1.4416\n",
      "Epoch 18 Batch 6 Loss 1.4589\n",
      "Epoch 18 Batch 7 Loss 1.4982\n",
      "Epoch 18 Batch 8 Loss 1.4818\n",
      "Epoch 18 Batch 9 Loss 1.4788\n",
      "Epoch 18 Batch 10 Loss 1.4611\n",
      "Epoch 18 Batch 11 Loss 1.4364\n",
      "Epoch 18 Batch 12 Loss 1.4427\n",
      "Epoch 18 Batch 13 Loss 1.4592\n",
      "Epoch 18 Batch 14 Loss 1.4992\n",
      "Epoch 18 Batch 15 Loss 1.4874\n",
      "Epoch 18 Batch 16 Loss 1.4835\n",
      "Epoch 18 Batch 17 Loss 1.4661\n",
      "Epoch 18 Batch 18 Loss 1.4387\n",
      "Epoch 18 Batch 19 Loss 1.4396\n",
      "Epoch 18 Batch 20 Loss 1.4519\n",
      "Epoch 18 Batch 21 Loss 1.4962\n",
      "Epoch 18 Batch 22 Loss 1.4853\n",
      "Epoch 18 Batch 23 Loss 1.4739\n",
      "Epoch 18 Batch 24 Loss 1.4557\n",
      "Epoch 18 Batch 25 Loss 1.4417\n",
      "Epoch 18 Batch 26 Loss 1.4427\n",
      "Epoch 18 Batch 27 Loss 1.4565\n",
      "Epoch 18 Batch 28 Loss 1.5008\n",
      "Epoch 18 Batch 29 Loss 1.4842\n",
      "Epoch 18 Batch 30 Loss 1.4665\n",
      "Epoch 18 Batch 31 Loss 1.4613\n",
      "Epoch 18 Batch 32 Loss 1.4396\n",
      "Epoch 18 Batch 33 Loss 1.4412\n",
      "Epoch 18 Batch 34 Loss 1.4555\n",
      "Epoch 18 Batch 35 Loss 1.4942\n",
      "Epoch 18 Batch 36 Loss 1.4749\n",
      "Epoch 18 Batch 37 Loss 1.4719\n",
      "Epoch 18 Batch 38 Loss 1.4743\n",
      "Epoch 18 Batch 39 Loss 1.4470\n",
      "Epoch 18 Batch 40 Loss 1.4476\n",
      "Epoch 18 Batch 41 Loss 1.4642\n",
      "Epoch 18 Batch 42 Loss 1.4928\n",
      "Epoch 18 Batch 43 Loss 1.4740\n",
      "Epoch 18 Batch 44 Loss 1.4822\n",
      "Epoch 18 Batch 45 Loss 1.4688\n",
      "Epoch 18 Batch 46 Loss 1.4389\n",
      "Epoch 18 Batch 47 Loss 1.4391\n",
      "Epoch 18 Batch 48 Loss 1.4549\n",
      "Epoch 18 Batch 49 Loss 1.4738\n",
      "Epoch 18 Batch 50 Loss 1.4926\n",
      "Epoch 18 Batch 51 Loss 1.4822\n",
      "Epoch 18 Batch 52 Loss 1.4710\n",
      "Epoch 18 Batch 53 Loss 1.4372\n",
      "Epoch 18 Batch 54 Loss 1.4343\n",
      "Epoch 18 Batch 55 Loss 1.4509\n",
      "Epoch 18 Batch 56 Loss 1.5183\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 18 Batch 57 Loss 1.5276\n",
      "Epoch 18 Batch 58 Loss 1.5108\n",
      "Epoch 18 Batch 59 Loss 1.5001\n",
      "14000\n",
      "Epoch 18 Loss 0.006296\n",
      "Time taken for 1 epoch 40.11274480819702 sec\n",
      "\n",
      "Epoch 19 Batch 0 Loss 1.5070\n",
      "Epoch 19 Batch 1 Loss 1.4874\n",
      "Epoch 19 Batch 2 Loss 1.4843\n",
      "Epoch 19 Batch 3 Loss 1.4590\n",
      "Epoch 19 Batch 4 Loss 1.4424\n",
      "Epoch 19 Batch 5 Loss 1.4394\n",
      "Epoch 19 Batch 6 Loss 1.4561\n",
      "Epoch 19 Batch 7 Loss 1.4940\n",
      "Epoch 19 Batch 8 Loss 1.4784\n",
      "Epoch 19 Batch 9 Loss 1.4756\n",
      "Epoch 19 Batch 10 Loss 1.4587\n",
      "Epoch 19 Batch 11 Loss 1.4341\n",
      "Epoch 19 Batch 12 Loss 1.4405\n",
      "Epoch 19 Batch 13 Loss 1.4573\n",
      "Epoch 19 Batch 14 Loss 1.4970\n",
      "Epoch 19 Batch 15 Loss 1.4852\n",
      "Epoch 19 Batch 16 Loss 1.4818\n",
      "Epoch 19 Batch 17 Loss 1.4643\n",
      "Epoch 19 Batch 18 Loss 1.4376\n",
      "Epoch 19 Batch 19 Loss 1.4389\n",
      "Epoch 19 Batch 20 Loss 1.4514\n",
      "Epoch 19 Batch 21 Loss 1.4950\n",
      "Epoch 19 Batch 22 Loss 1.4841\n",
      "Epoch 19 Batch 23 Loss 1.4728\n",
      "Epoch 19 Batch 24 Loss 1.4550\n",
      "Epoch 19 Batch 25 Loss 1.4415\n",
      "Epoch 19 Batch 26 Loss 1.4424\n",
      "Epoch 19 Batch 27 Loss 1.4561\n",
      "Epoch 19 Batch 28 Loss 1.4997\n",
      "Epoch 19 Batch 29 Loss 1.4832\n",
      "Epoch 19 Batch 30 Loss 1.4658\n",
      "Epoch 19 Batch 31 Loss 1.4614\n",
      "Epoch 19 Batch 32 Loss 1.4400\n",
      "Epoch 19 Batch 33 Loss 1.4414\n",
      "Epoch 19 Batch 34 Loss 1.4557\n",
      "Epoch 19 Batch 35 Loss 1.4935\n",
      "Epoch 19 Batch 36 Loss 1.4746\n",
      "Epoch 19 Batch 37 Loss 1.4706\n",
      "Epoch 19 Batch 38 Loss 1.4720\n",
      "Epoch 19 Batch 39 Loss 1.4445\n",
      "Epoch 19 Batch 40 Loss 1.4454\n",
      "Epoch 19 Batch 41 Loss 1.4615\n",
      "Epoch 19 Batch 42 Loss 1.4903\n",
      "Epoch 19 Batch 43 Loss 1.4721\n",
      "Epoch 19 Batch 44 Loss 1.4808\n",
      "Epoch 19 Batch 45 Loss 1.4671\n",
      "Epoch 19 Batch 46 Loss 1.4375\n",
      "Epoch 19 Batch 47 Loss 1.4380\n",
      "Epoch 19 Batch 48 Loss 1.4544\n",
      "Epoch 19 Batch 49 Loss 1.4733\n",
      "Epoch 19 Batch 50 Loss 1.4917\n",
      "Epoch 19 Batch 51 Loss 1.4811\n",
      "Epoch 19 Batch 52 Loss 1.4698\n",
      "Epoch 19 Batch 53 Loss 1.4366\n",
      "Epoch 19 Batch 54 Loss 1.4347\n",
      "Epoch 19 Batch 55 Loss 1.4511\n",
      "Epoch 19 Batch 56 Loss 1.5183\n",
      "Epoch 19 Batch 57 Loss 1.5267\n",
      "Epoch 19 Batch 58 Loss 1.5101\n",
      "Epoch 19 Batch 59 Loss 1.4994\n",
      "14000\n",
      "Epoch 19 Loss 0.006290\n",
      "Time taken for 1 epoch 38.220736026763916 sec\n",
      "\n",
      "Epoch 20 Batch 0 Loss 1.5044\n",
      "Epoch 20 Batch 1 Loss 1.4850\n",
      "Epoch 20 Batch 2 Loss 1.4818\n",
      "Epoch 20 Batch 3 Loss 1.4574\n",
      "Epoch 20 Batch 4 Loss 1.4414\n",
      "Epoch 20 Batch 5 Loss 1.4383\n",
      "Epoch 20 Batch 6 Loss 1.4544\n",
      "Epoch 20 Batch 7 Loss 1.4911\n",
      "Epoch 20 Batch 8 Loss 1.4760\n",
      "Epoch 20 Batch 9 Loss 1.4733\n",
      "Epoch 20 Batch 10 Loss 1.4569\n",
      "Epoch 20 Batch 11 Loss 1.4326\n",
      "Epoch 20 Batch 12 Loss 1.4392\n",
      "Epoch 20 Batch 13 Loss 1.4560\n",
      "Epoch 20 Batch 14 Loss 1.4953\n",
      "Epoch 20 Batch 15 Loss 1.4835\n",
      "Epoch 20 Batch 16 Loss 1.4804\n",
      "Epoch 20 Batch 17 Loss 1.4627\n",
      "Epoch 20 Batch 18 Loss 1.4368\n",
      "Epoch 20 Batch 19 Loss 1.4383\n",
      "Epoch 20 Batch 20 Loss 1.4508\n",
      "Epoch 20 Batch 21 Loss 1.4940\n",
      "Epoch 20 Batch 22 Loss 1.4830\n",
      "Epoch 20 Batch 23 Loss 1.4715\n",
      "Epoch 20 Batch 24 Loss 1.4540\n",
      "Epoch 20 Batch 25 Loss 1.4412\n",
      "Epoch 20 Batch 26 Loss 1.4422\n",
      "Epoch 20 Batch 27 Loss 1.4558\n",
      "Epoch 20 Batch 28 Loss 1.4989\n",
      "Epoch 20 Batch 29 Loss 1.4822\n",
      "Epoch 20 Batch 30 Loss 1.4652\n",
      "Epoch 20 Batch 31 Loss 1.4614\n",
      "Epoch 20 Batch 32 Loss 1.4405\n",
      "Epoch 20 Batch 33 Loss 1.4416\n",
      "Epoch 20 Batch 34 Loss 1.4561\n",
      "Epoch 20 Batch 35 Loss 1.4930\n",
      "Epoch 20 Batch 36 Loss 1.4742\n",
      "Epoch 20 Batch 37 Loss 1.4696\n",
      "Epoch 20 Batch 38 Loss 1.4705\n",
      "Epoch 20 Batch 39 Loss 1.4430\n",
      "Epoch 20 Batch 40 Loss 1.4440\n",
      "Epoch 20 Batch 41 Loss 1.4597\n",
      "Epoch 20 Batch 42 Loss 1.4885\n",
      "Epoch 20 Batch 43 Loss 1.4705\n",
      "Epoch 20 Batch 44 Loss 1.4794\n",
      "Epoch 20 Batch 45 Loss 1.4656\n",
      "Epoch 20 Batch 46 Loss 1.4362\n",
      "Epoch 20 Batch 47 Loss 1.4368\n",
      "Epoch 20 Batch 48 Loss 1.4535\n",
      "Epoch 20 Batch 49 Loss 1.4724\n",
      "Epoch 20 Batch 50 Loss 1.4905\n",
      "Epoch 20 Batch 51 Loss 1.4799\n",
      "Epoch 20 Batch 52 Loss 1.4686\n",
      "Epoch 20 Batch 53 Loss 1.4357\n",
      "Epoch 20 Batch 54 Loss 1.4347\n",
      "Epoch 20 Batch 55 Loss 1.4509\n",
      "Epoch 20 Batch 56 Loss 1.5184\n",
      "Epoch 20 Batch 57 Loss 1.5263\n",
      "Epoch 20 Batch 58 Loss 1.5098\n",
      "Epoch 20 Batch 59 Loss 1.4989\n",
      "14000\n",
      "Epoch 20 Loss 0.006285\n",
      "Time taken for 1 epoch 40.58538866043091 sec\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "EPOCHS = 20\n",
    "\n",
    "for epoch in range(start_epoch, EPOCHS):\n",
    "    start = time.time()\n",
    "    total_loss = 0\n",
    "\n",
    "    for (batch, (img_tensor, target)) in enumerate(image_dataset_encoded):\n",
    "      #print(img_tensor.shape)\n",
    "      #print(target.shape)\n",
    "        batch_loss, t_loss = train_step(img_tensor, target)\n",
    "        total_loss += t_loss\n",
    "\n",
    "        if batch % 1 == 0:\n",
    "            print ('Epoch {} Batch {} Loss {:.4f}'.format(\n",
    "                epoch + 1, batch, batch_loss / int(target.shape[1])))\n",
    "    # storing the epoch end loss value to plot later\n",
    "    loss_plot.append(total_loss / num_steps)\n",
    "\n",
    "    if epoch % 5 == 0:\n",
    "        ckpt_manager.save() \n",
    "    print(num_steps)\n",
    "    print ('Epoch {} Loss {:.6f}'.format(epoch + 1,\n",
    "                                         total_loss/num_steps))\n",
    "    print ('Time taken for 1 epoch {} sec\\n'.format(time.time() - start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 295
    },
    "executionInfo": {
     "elapsed": 5485,
     "status": "ok",
     "timestamp": 1605397126122,
     "user": {
      "displayName": "Morgan Allen",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjaNMWmr2TlLrY_MBSDquiSqhgTuK8vCNzY4tLH=s64",
      "userId": "12026828979516915336"
     },
     "user_tz": 360
    },
    "id": "lsSx6FGZZh3q",
    "outputId": "94b1a0e7-25ef-4bb5-e1ed-cdd2b0c113eb"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(loss_plot)\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Loss Plot')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "S60WeHNwdPDG"
   },
   "source": [
    "# Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "id": "31N4XxAYcahR"
   },
   "outputs": [],
   "source": [
    "def evaluate(image):\n",
    "    attention_plot = np.zeros((max_length, attention_features_shape))\n",
    "\n",
    "    hidden = decoder.reset_state(batch_size=1)\n",
    "\n",
    "    temp_input = tf.expand_dims(image, 0)\n",
    "    img_tensor_val = image_features_extract_model(temp_input)\n",
    "    img_tensor_val = tf.reshape(img_tensor_val, (img_tensor_val.shape[0], -1, img_tensor_val.shape[3]))\n",
    "\n",
    "    features = encoder(img_tensor_val)\n",
    "\n",
    "    dec_input = tf.expand_dims([tokenizer.word_index['\\t']], 0)\n",
    "    result = []\n",
    "\n",
    "    for i in range(max_length):\n",
    "        predictions, hidden, attention_weights = decoder(dec_input, features, hidden)\n",
    "        attention_plot[i] = tf.reshape(attention_weights, (-1, ))\n",
    "\n",
    "        predicted_id = tf.random.categorical(predictions, 1)[0][0]\n",
    "        predicted_id = int(predicted_id)\n",
    "        result.append(tokenizer.index_word[predicted_id])\n",
    "\n",
    "        if tokenizer.index_word[predicted_id] == '\\n':\n",
    "            return result, attention_plot\n",
    "\n",
    "        dec_input = tf.expand_dims([predicted_id], 0)\n",
    "\n",
    "    attention_plot = attention_plot[:len(result), :]\n",
    "    return result, attention_plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_img = img_mini[16000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "id": "CEEvzEL8dmsV"
   },
   "outputs": [],
   "source": [
    "def plot_attention(image, result, attention_plot):\n",
    "    temp_image = np.array(image)\n",
    "\n",
    "    fig = plt.figure(figsize=(10, 10))\n",
    "\n",
    "    len_result = len(result)\n",
    "    for l in range(len_result):\n",
    "        temp_att = np.resize(attention_plot[l], (8, 8))\n",
    "        ax = fig.add_subplot(len_result//2, len_result//2, l+1)\n",
    "        ax.set_title(result[l])\n",
    "        img = ax.imshow(temp_image)\n",
    "        ax.imshow(temp_att, cmap='gray', alpha=0.6, extent=img.get_extent())\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "p-GvV9Q9eaJh"
   },
   "source": [
    "Run attention_plot to see what it looks like"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 822
    },
    "executionInfo": {
     "elapsed": 1655,
     "status": "ok",
     "timestamp": 1605397136287,
     "user": {
      "displayName": "Morgan Allen",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjaNMWmr2TlLrY_MBSDquiSqhgTuK8vCNzY4tLH=s64",
      "userId": "12026828979516915336"
     },
     "user_tz": 360
    },
    "id": "0aBJHIb3dpoo",
    "outputId": "6fc5ed8d-0585-409a-a143-8cd23b15d7b0"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction Caption: 2 y + 2 \n",
      "\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "num must be 1 <= num <= 4, not 5",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-50-4ef9816bafa7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattention_plot\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_img\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m'Prediction Caption:'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m' '\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mplot_attention\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_img\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattention_plot\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;31m# opening the image\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_img\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-49-d19545798c7a>\u001b[0m in \u001b[0;36mplot_attention\u001b[0;34m(image, result, attention_plot)\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0ml\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen_result\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m         \u001b[0mtemp_att\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mattention_plot\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0ml\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m8\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m8\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m         \u001b[0max\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_subplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen_result\u001b[0m\u001b[0;34m//\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen_result\u001b[0m\u001b[0;34m//\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ml\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m         \u001b[0max\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_title\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0ml\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m         \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0max\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtemp_image\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/miniconda3/lib/python3.7/site-packages/matplotlib/figure.py\u001b[0m in \u001b[0;36madd_subplot\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1400\u001b[0m                     \u001b[0;31m# more similar to add_axes.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1401\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_axstack\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mremove\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0max\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1402\u001b[0;31m             \u001b[0max\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msubplot_class_factory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprojection_class\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1403\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1404\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_add_axes_internal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0max\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/miniconda3/lib/python3.7/site-packages/matplotlib/axes/_subplots.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, fig, *args, **kwargs)\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfigure\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfig\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 39\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_subplotspec\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSubplotSpec\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_from_subplot_args\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     40\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate_params\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m         \u001b[0;31m# _axes_class is set in the subplot_class_factory\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/miniconda3/lib/python3.7/site-packages/matplotlib/gridspec.py\u001b[0m in \u001b[0;36m_from_subplot_args\u001b[0;34m(figure, args)\u001b[0m\n\u001b[1;32m    688\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mnum\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mnum\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0mrows\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mcols\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    689\u001b[0m                     raise ValueError(\n\u001b[0;32m--> 690\u001b[0;31m                         f\"num must be 1 <= num <= {rows*cols}, not {num}\")\n\u001b[0m\u001b[1;32m    691\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mgs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnum\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m   \u001b[0;31m# -1 due to MATLAB indexing.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    692\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: num must be 1 <= num <= 4, not 5"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x720 with 4 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "result, attention_plot = evaluate(test_img)\n",
    "print ('Prediction Caption:', ' '.join(result))\n",
    "plot_attention(test_img, result, attention_plot)\n",
    "# opening the image\n",
    "plt.imshow(test_img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Morgan Copy Image Captioning with Attention (Drive).ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
