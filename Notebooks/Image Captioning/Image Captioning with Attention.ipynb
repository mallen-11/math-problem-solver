{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Image Captioning with Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "from PIL import Image\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import plot_confusion_matrix, confusion_matrix\n",
    "\n",
    "from tensorflow.keras.layers import Dense, Flatten, Conv2D, MaxPooling2D, Dropout, BatchNormalization\n",
    "from tensorflow.keras import Model\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow import image\n",
    "\n",
    "import itertools\n",
    "\n",
    "tf.random.set_seed(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading the Data. The data consist of pictures and labels of linear functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import os\n",
    "\n",
    "def load_images_from_folder(folder):\n",
    "    images = []\n",
    "    for filename in os.listdir(folder):\n",
    "        img = cv2.imread(os.path.join(folder,filename))\n",
    "        if img is not None:\n",
    "            images.append(img)\n",
    "    return images\n",
    "folder=\"../../linear_fcns/images\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "images = load_images_from_folder(folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = pd.read_csv('../../linear_fcns/data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_array = labels['latex'].to_numpy()\n",
    "labels_mini = label_array[:6000]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the labels we need to add start and end tokens so the model can recognize what to write."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "for val in range(len(labels_mini)):\n",
    "    labels_mini[val] = f'<start> {labels_mini[val]} <end>'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's reshape the images so we can plot them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6000, 72, 360, 1)"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "images_mini = images[:6000]\n",
    "images_mini = np.array(images_mini)\n",
    "images_mini = 255 - images_mini\n",
    "images_mini = tf.image.rgb_to_grayscale(images_mini)\n",
    "images_mini = np.array(images_mini)\n",
    "images_mini.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7fc292b3a550>"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(images_mini[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can process them and prepare them for inceptionv3 which is transfer learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_mini = tf.image.resize_with_pad(images_mini, 299, 299)\n",
    "img_mini = tf.keras.applications.inception_v3.preprocess_input(img_mini)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7fc2929e7710>"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(img_mini[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can load the features of the inceptionv3 model as seen below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_model = tf.keras.applications.InceptionV3(include_top=False,\n",
    "                                                weights='imagenet')\n",
    "new_input = image_model.input\n",
    "hidden_layer = image_model.layers[-1].output\n",
    "\n",
    "image_features_extract_model = tf.keras.Model(new_input, hidden_layer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the part that I am messing up and I do not understand how to fix it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "ename": "InvalidArgumentError",
     "evalue": "input must be 4-dimensional[72,360,1] [Op:Conv2D]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-57-bcee0539fef1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mimg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mimage_dataset\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m   \u001b[0mbatch_features\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimage_features_extract_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m   batch_features = tf.reshape(batch_features,\n\u001b[1;32m     10\u001b[0m                               (batch_features.shape[0], -1, batch_features.shape[3]))\n",
      "\u001b[0;32m/opt/miniconda3/lib/python3.7/site-packages/tensorflow/python/keras/engine/base_layer.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    983\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    984\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menable_auto_cast_variables\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compute_dtype_object\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 985\u001b[0;31m           \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcall_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    986\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    987\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_activity_regularizer\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/miniconda3/lib/python3.7/site-packages/tensorflow/python/keras/engine/functional.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, inputs, training, mask)\u001b[0m\n\u001b[1;32m    384\u001b[0m     \"\"\"\n\u001b[1;32m    385\u001b[0m     return self._run_internal_graph(\n\u001b[0;32m--> 386\u001b[0;31m         inputs, training=training, mask=mask)\n\u001b[0m\u001b[1;32m    387\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    388\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mcompute_output_shape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_shape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/miniconda3/lib/python3.7/site-packages/tensorflow/python/keras/engine/functional.py\u001b[0m in \u001b[0;36m_run_internal_graph\u001b[0;34m(self, inputs, training, mask)\u001b[0m\n\u001b[1;32m    506\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    507\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnode\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap_arguments\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 508\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnode\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    509\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    510\u001b[0m         \u001b[0;31m# Update tensor_dict.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/miniconda3/lib/python3.7/site-packages/tensorflow/python/keras/engine/base_layer.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    983\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    984\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menable_auto_cast_variables\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compute_dtype_object\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 985\u001b[0;31m           \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcall_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    986\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    987\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_activity_regularizer\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/miniconda3/lib/python3.7/site-packages/tensorflow/python/keras/layers/convolutional.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m    245\u001b[0m       \u001b[0minputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0marray_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compute_causal_padding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    246\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 247\u001b[0;31m     \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_convolution_op\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkernel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    248\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    249\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muse_bias\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/miniconda3/lib/python3.7/site-packages/tensorflow/python/util/dispatch.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    199\u001b[0m     \u001b[0;34m\"\"\"Call target, and fall back on dispatchers if there is a TypeError.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    200\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 201\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    202\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mTypeError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    203\u001b[0m       \u001b[0;31m# Note: convert_to_eager_tensor currently raises a ValueError, not a\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/miniconda3/lib/python3.7/site-packages/tensorflow/python/ops/nn_ops.py\u001b[0m in \u001b[0;36mconvolution_v2\u001b[0;34m(input, filters, strides, padding, data_format, dilations, name)\u001b[0m\n\u001b[1;32m   1016\u001b[0m       \u001b[0mdata_format\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdata_format\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1017\u001b[0m       \u001b[0mdilations\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdilations\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1018\u001b[0;31m       name=name)\n\u001b[0m\u001b[1;32m   1019\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1020\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/miniconda3/lib/python3.7/site-packages/tensorflow/python/ops/nn_ops.py\u001b[0m in \u001b[0;36mconvolution_internal\u001b[0;34m(input, filters, strides, padding, data_format, dilations, name, call_from_convolution, num_spatial_dims)\u001b[0m\n\u001b[1;32m   1146\u001b[0m           \u001b[0mdata_format\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdata_format\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1147\u001b[0m           \u001b[0mdilations\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdilations\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1148\u001b[0;31m           name=name)\n\u001b[0m\u001b[1;32m   1149\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1150\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mchannel_index\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/miniconda3/lib/python3.7/site-packages/tensorflow/python/ops/nn_ops.py\u001b[0m in \u001b[0;36m_conv2d_expanded_batch\u001b[0;34m(input, filters, strides, padding, data_format, dilations, name)\u001b[0m\n\u001b[1;32m   2590\u001b[0m         \u001b[0mdata_format\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdata_format\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2591\u001b[0m         \u001b[0mdilations\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdilations\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2592\u001b[0;31m         name=name)\n\u001b[0m\u001b[1;32m   2593\u001b[0m   return squeeze_batch_dims(\n\u001b[1;32m   2594\u001b[0m       \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/miniconda3/lib/python3.7/site-packages/tensorflow/python/ops/gen_nn_ops.py\u001b[0m in \u001b[0;36mconv2d\u001b[0;34m(input, filter, strides, padding, use_cudnn_on_gpu, explicit_paddings, data_format, dilations, name)\u001b[0m\n\u001b[1;32m    936\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0m_result\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    937\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0m_core\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 938\u001b[0;31m       \u001b[0m_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraise_from_not_ok_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    939\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0m_core\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_FallbackException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    940\u001b[0m       \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/miniconda3/lib/python3.7/site-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36mraise_from_not_ok_status\u001b[0;34m(e, name)\u001b[0m\n\u001b[1;32m   6841\u001b[0m   \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m\" name: \"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m\"\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6842\u001b[0m   \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 6843\u001b[0;31m   \u001b[0msix\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraise_from\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_status_to_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   6844\u001b[0m   \u001b[0;31m# pylint: enable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6845\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/miniconda3/lib/python3.7/site-packages/six.py\u001b[0m in \u001b[0;36mraise_from\u001b[0;34m(value, from_value)\u001b[0m\n",
      "\u001b[0;31mInvalidArgumentError\u001b[0m: input must be 4-dimensional[72,360,1] [Op:Conv2D]"
     ]
    }
   ],
   "source": [
    "# Get unique images\n",
    "encode_train = images_mini\n",
    "\n",
    "# Feel free to change batch_size according to your system configuration\n",
    "image_dataset = tf.data.Dataset.from_tensor_slices(encode_train)\n",
    "\n",
    "for img in image_dataset:\n",
    "  batch_features = image_features_extract_model(img)\n",
    "  batch_features = tf.reshape(batch_features,\n",
    "                              (batch_features.shape[0], -1, batch_features.shape[3]))\n",
    "\n",
    "  for bf, p in zip(batch_features, path):\n",
    "    path_of_feature = p.numpy().decode(\"utf-8\")\n",
    "    np.save(path_of_feature, bf.numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenizing the labels\n",
    "\n",
    "Now we can pad the labels to make sure they are all the same length."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the maximum length of any caption in our dataset\n",
    "def calc_max_length(tensor):\n",
    "    return max(len(t) for t in tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose the top 5000 words from the vocabulary\n",
    "train_seqs = tokenizer.texts_to_sequences(labels_mini)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.word_index['<pad>'] = 0\n",
    "tokenizer.index_word[0] = '<pad>'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the tokenized vectors\n",
    "train_seqs = tokenizer.texts_to_sequences(labels_mini)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pad each vector to the max_length of the captions\n",
    "# If you do not provide a max_length value, pad_sequences calculates it automatically\n",
    "cap_vector = tf.keras.preprocessing.sequence.pad_sequences(train_seqs, padding='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculates the max_length, which is used to store the attention weights\n",
    "max_length = calc_max_length(train_seqs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train Test Split\n",
    "\n",
    "This kind of makes a train test split but I don't think it is exactly what we want."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_train = img_mini[:4000]\n",
    "label_train = labels_mini[:4000]\n",
    "img_test = img_mini[4000:]\n",
    "label_test = labels_mini[4000:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feel free to change these parameters according to your system's configuration\n",
    "\n",
    "BATCH_SIZE = 64\n",
    "BUFFER_SIZE = 1000\n",
    "embedding_dim = 256\n",
    "units = 512\n",
    "vocab_size = top_k + 1\n",
    "num_steps = len(img_train) // BATCH_SIZE\n",
    "# Shape of the vector extracted from InceptionV3 is (64, 2048)\n",
    "# These two variables represent that vector shape\n",
    "features_shape = 2048\n",
    "attention_features_shape = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the numpy files\n",
    "def map_func(img_name, cap):\n",
    "    img_tensor = np.load(img_name.decode('utf-8')+'.npy')\n",
    "    return img_tensor, cap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = tf.data.Dataset.from_tensor_slices((img_train, label_train))\n",
    "\n",
    "# Use map to load the numpy files in parallel\n",
    "dataset = dataset.map(lambda item1, item2: tf.numpy_function(\n",
    "          map_func, [item1, item2], [tf.float32, tf.int32]),\n",
    "          num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
    "\n",
    "# Shuffle and batch\n",
    "dataset = dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE)\n",
    "dataset = dataset.prefetch(buffer_size=tf.data.experimental.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model\n",
    "\n",
    "Below we are defining the attention, the encoder and the decoder. The encoder is just a fully connected layer from the features already extracted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BahdanauAttention(tf.keras.Model):\n",
    "    def __init__(self, units):\n",
    "        super(BahdanauAttention, self).__init__()\n",
    "        self.W1 = tf.keras.layers.Dense(units)\n",
    "        self.W2 = tf.keras.layers.Dense(units)\n",
    "        self.V = tf.keras.layers.Dense(1)\n",
    "\n",
    "    def call(self, features, hidden):\n",
    "        # features(CNN_encoder output) shape == (batch_size, 64, embedding_dim)\n",
    "\n",
    "        # hidden shape == (batch_size, hidden_size)\n",
    "        # hidden_with_time_axis shape == (batch_size, 1, hidden_size)\n",
    "        hidden_with_time_axis = tf.expand_dims(hidden, 1)\n",
    "\n",
    "        # attention_hidden_layer shape == (batch_size, 64, units)\n",
    "        attention_hidden_layer = (tf.nn.tanh(self.W1(features) +\n",
    "                                             self.W2(hidden_with_time_axis)))\n",
    "\n",
    "        # score shape == (batch_size, 64, 1)\n",
    "        # This gives you an unnormalized score for each image feature.\n",
    "        score = self.V(attention_hidden_layer)\n",
    "\n",
    "        # attention_weights shape == (batch_size, 64, 1)\n",
    "        attention_weights = tf.nn.softmax(score, axis=1)\n",
    "\n",
    "        # context_vector shape after sum == (batch_size, hidden_size)\n",
    "        context_vector = attention_weights * features\n",
    "        context_vector = tf.reduce_sum(context_vector, axis=1)\n",
    "\n",
    "        return context_vector, attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN_Encoder(tf.keras.Model):\n",
    "    # Since you have already extracted the features and dumped it using pickle\n",
    "    # This encoder passes those features through a Fully connected layer\n",
    "    def __init__(self, embedding_dim):\n",
    "        super(CNN_Encoder, self).__init__()\n",
    "        # shape after fc == (batch_size, 64, embedding_dim)\n",
    "        self.fc = tf.keras.layers.Dense(embedding_dim)\n",
    "\n",
    "    def call(self, x):\n",
    "        x = self.fc(x)\n",
    "        x = tf.nn.relu(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNN_Decoder(tf.keras.Model):\n",
    "    def __init__(self, embedding_dim, units, vocab_size):\n",
    "        super(RNN_Decoder, self).__init__()\n",
    "        self.units = units\n",
    "\n",
    "        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
    "        self.gru = tf.keras.layers.GRU(self.units,\n",
    "                                       return_sequences=True,\n",
    "                                       return_state=True,\n",
    "                                       recurrent_initializer='glorot_uniform')\n",
    "        self.fc1 = tf.keras.layers.Dense(self.units)\n",
    "        self.fc2 = tf.keras.layers.Dense(vocab_size)\n",
    "\n",
    "        self.attention = BahdanauAttention(self.units)\n",
    "\n",
    "    def call(self, x, features, hidden):\n",
    "        # defining attention as a separate model\n",
    "        context_vector, attention_weights = self.attention(features, hidden)\n",
    "\n",
    "        # x shape after passing through embedding == (batch_size, 1, embedding_dim)\n",
    "        x = self.embedding(x)\n",
    "\n",
    "        # x shape after concatenation == (batch_size, 1, embedding_dim + hidden_size)\n",
    "        x = tf.concat([tf.expand_dims(context_vector, 1), x], axis=-1)\n",
    "\n",
    "        # passing the concatenated vector to the GRU\n",
    "        output, state = self.gru(x)\n",
    "\n",
    "        # shape == (batch_size, max_length, hidden_size)\n",
    "        x = self.fc1(output)\n",
    "\n",
    "        # x shape == (batch_size * max_length, hidden_size)\n",
    "        x = tf.reshape(x, (-1, x.shape[2]))\n",
    "\n",
    "        # output shape == (batch_size * max_length, vocab)\n",
    "        x = self.fc2(x)\n",
    "\n",
    "        return x, state, attention_weights\n",
    "\n",
    "    def reset_state(self, batch_size):\n",
    "        return tf.zeros((batch_size, self.units))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = CNN_Encoder(embedding_dim)\n",
    "decoder = RNN_Decoder(embedding_dim, units, vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tf.keras.optimizers.Adam()\n",
    "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(\n",
    "    from_logits=True, reduction='none')\n",
    "\n",
    "def loss_function(real, pred):\n",
    "    mask = tf.math.logical_not(tf.math.equal(real, 0))\n",
    "    loss_ = loss_object(real, pred)\n",
    "\n",
    "    mask = tf.cast(mask, dtype=loss_.dtype)\n",
    "    loss_ *= mask\n",
    "\n",
    "    return tf.reduce_mean(loss_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_path = \"./checkpoints/train\"\n",
    "ckpt = tf.train.Checkpoint(encoder=encoder,\n",
    "                           decoder=decoder,\n",
    "                           optimizer = optimizer)\n",
    "ckpt_manager = tf.train.CheckpointManager(ckpt, checkpoint_path, max_to_keep=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_epoch = 0\n",
    "if ckpt_manager.latest_checkpoint:\n",
    "      start_epoch = int(ckpt_manager.latest_checkpoint.split('-')[-1])\n",
    "      # restoring the latest checkpoint in checkpoint_path\n",
    "      ckpt.restore(ckpt_manager.latest_checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "# adding this in a separate cell because if you run the training cell\n",
    "# many times, the loss_plot array will be reset\n",
    "loss_plot = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def train_step(img_tensor, target):\n",
    "    loss = 0\n",
    "\n",
    "          # initializing the hidden state for each batch\n",
    "          # because the captions are not related from image to image\n",
    "    hidden = decoder.reset_state(batch_size=target.shape[0])\n",
    "\n",
    "    dec_input = tf.expand_dims([tokenizer.word_index['<start>']] * target.shape[0], 1)\n",
    "\n",
    "    with tf.GradientTape() as tape:\n",
    "        features = encoder(img_tensor)\n",
    "\n",
    "        for i in range(1, target.shape[1]):\n",
    "                  # passing the features through the decoder\n",
    "            predictions, hidden, _ = decoder(dec_input, features, hidden)\n",
    "\n",
    "            loss += loss_function(target[:, i], predictions)\n",
    "\n",
    "                  # using teacher forcing\n",
    "            dec_input = tf.expand_dims(target[:, i], 1)\n",
    "\n",
    "    total_loss = (loss / int(target.shape[1]))\n",
    "\n",
    "    trainable_variables = encoder.trainable_variables + decoder.trainable_variables\n",
    "\n",
    "    gradients = tape.gradient(loss, trainable_variables)\n",
    "\n",
    "    optimizer.apply_gradients(zip(gradients, trainable_variables))\n",
    "\n",
    "    return loss, total_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "ename": "UnknownError",
     "evalue": "AttributeError: 'numpy.ndarray' object has no attribute 'decode'\nTraceback (most recent call last):\n\n  File \"/opt/miniconda3/lib/python3.7/site-packages/tensorflow/python/ops/script_ops.py\", line 244, in __call__\n    ret = func(*args)\n\n  File \"/opt/miniconda3/lib/python3.7/site-packages/tensorflow/python/autograph/impl/api.py\", line 302, in wrapper\n    return func(*args, **kwargs)\n\n  File \"<ipython-input-65-f8ebf6c66200>\", line 3, in map_func\n    img_tensor = np.load(img_name.decode('utf-8')+'.npy')\n\nAttributeError: 'numpy.ndarray' object has no attribute 'decode'\n\n\n\t [[{{node PyFunc}}]]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mUnknownError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m/opt/miniconda3/lib/python3.7/site-packages/tensorflow/python/eager/context.py\u001b[0m in \u001b[0;36mexecution_mode\u001b[0;34m(mode)\u001b[0m\n\u001b[1;32m   2101\u001b[0m       \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecutor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mexecutor_new\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2102\u001b[0;31m       \u001b[0;32myield\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2103\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/miniconda3/lib/python3.7/site-packages/tensorflow/python/data/ops/iterator_ops.py\u001b[0m in \u001b[0;36m_next_internal\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    757\u001b[0m             \u001b[0moutput_types\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_flat_output_types\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 758\u001b[0;31m             output_shapes=self._flat_output_shapes)\n\u001b[0m\u001b[1;32m    759\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/miniconda3/lib/python3.7/site-packages/tensorflow/python/ops/gen_dataset_ops.py\u001b[0m in \u001b[0;36miterator_get_next\u001b[0;34m(iterator, output_types, output_shapes, name)\u001b[0m\n\u001b[1;32m   2609\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0m_core\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2610\u001b[0;31m       \u001b[0m_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraise_from_not_ok_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2611\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0m_core\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_FallbackException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/miniconda3/lib/python3.7/site-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36mraise_from_not_ok_status\u001b[0;34m(e, name)\u001b[0m\n\u001b[1;32m   6842\u001b[0m   \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 6843\u001b[0;31m   \u001b[0msix\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraise_from\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_status_to_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   6844\u001b[0m   \u001b[0;31m# pylint: enable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/miniconda3/lib/python3.7/site-packages/six.py\u001b[0m in \u001b[0;36mraise_from\u001b[0;34m(value, from_value)\u001b[0m\n",
      "\u001b[0;31mUnknownError\u001b[0m: AttributeError: 'numpy.ndarray' object has no attribute 'decode'\nTraceback (most recent call last):\n\n  File \"/opt/miniconda3/lib/python3.7/site-packages/tensorflow/python/ops/script_ops.py\", line 244, in __call__\n    ret = func(*args)\n\n  File \"/opt/miniconda3/lib/python3.7/site-packages/tensorflow/python/autograph/impl/api.py\", line 302, in wrapper\n    return func(*args, **kwargs)\n\n  File \"<ipython-input-65-f8ebf6c66200>\", line 3, in map_func\n    img_tensor = np.load(img_name.decode('utf-8')+'.npy')\n\nAttributeError: 'numpy.ndarray' object has no attribute 'decode'\n\n\n\t [[{{node PyFunc}}]] [Op:IteratorGetNext]",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mUnknownError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-80-90a72f0741ff>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mtotal_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mimg_tensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m         \u001b[0mbatch_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg_tensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m         \u001b[0mtotal_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mt_loss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/miniconda3/lib/python3.7/site-packages/tensorflow/python/data/ops/iterator_ops.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    734\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    735\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m__next__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# For Python 3 compatibility\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 736\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    737\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    738\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_next_internal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/miniconda3/lib/python3.7/site-packages/tensorflow/python/data/ops/iterator_ops.py\u001b[0m in \u001b[0;36mnext\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    770\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    771\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 772\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_internal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    773\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOutOfRangeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    774\u001b[0m       \u001b[0;32mraise\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/miniconda3/lib/python3.7/site-packages/tensorflow/python/data/ops/iterator_ops.py\u001b[0m in \u001b[0;36m_next_internal\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    762\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_element_spec\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_from_compatible_tensor_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mret\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    763\u001b[0m       \u001b[0;32mexcept\u001b[0m \u001b[0mAttributeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 764\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mstructure\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_compatible_tensor_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_element_spec\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mret\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    765\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    766\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/miniconda3/lib/python3.7/contextlib.py\u001b[0m in \u001b[0;36m__exit__\u001b[0;34m(self, type, value, traceback)\u001b[0m\n\u001b[1;32m    128\u001b[0m                 \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    129\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 130\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgen\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mthrow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraceback\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    131\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mexc\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    132\u001b[0m                 \u001b[0;31m# Suppress StopIteration *unless* it's the same exception that\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/miniconda3/lib/python3.7/site-packages/tensorflow/python/eager/context.py\u001b[0m in \u001b[0;36mexecution_mode\u001b[0;34m(mode)\u001b[0m\n\u001b[1;32m   2103\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2104\u001b[0m       \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecutor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mexecutor_old\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2105\u001b[0;31m       \u001b[0mexecutor_new\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2106\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2107\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/miniconda3/lib/python3.7/site-packages/tensorflow/python/eager/executor.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     65\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m     \u001b[0;34m\"\"\"Waits for ops dispatched in this executor to finish.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 67\u001b[0;31m     \u001b[0mpywrap_tfe\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTFE_ExecutorWaitForAllPendingNodes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     68\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mclear_error\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mUnknownError\u001b[0m: AttributeError: 'numpy.ndarray' object has no attribute 'decode'\nTraceback (most recent call last):\n\n  File \"/opt/miniconda3/lib/python3.7/site-packages/tensorflow/python/ops/script_ops.py\", line 244, in __call__\n    ret = func(*args)\n\n  File \"/opt/miniconda3/lib/python3.7/site-packages/tensorflow/python/autograph/impl/api.py\", line 302, in wrapper\n    return func(*args, **kwargs)\n\n  File \"<ipython-input-65-f8ebf6c66200>\", line 3, in map_func\n    img_tensor = np.load(img_name.decode('utf-8')+'.npy')\n\nAttributeError: 'numpy.ndarray' object has no attribute 'decode'\n\n\n\t [[{{node PyFunc}}]]"
     ]
    }
   ],
   "source": [
    "import time\n",
    "EPOCHS = 20\n",
    "\n",
    "for epoch in range(start_epoch, EPOCHS):\n",
    "    start = time.time()\n",
    "    total_loss = 0\n",
    "\n",
    "    for (batch, (img_tensor, target)) in enumerate(dataset):\n",
    "        batch_loss, t_loss = train_step(img_tensor, target)\n",
    "        total_loss += t_loss\n",
    "\n",
    "        if batch % 100 == 0:\n",
    "            print ('Epoch {} Batch {} Loss {:.4f}'.format(\n",
    "              epoch + 1, batch, batch_loss.numpy() / int(target.shape[1])))\n",
    "    # storing the epoch end loss value to plot later\n",
    "    loss_plot.append(total_loss / num_steps)\n",
    "\n",
    "    if epoch % 5 == 0:\n",
    "        ckpt_manager.save()\n",
    "\n",
    "    print ('Epoch {} Loss {:.6f}'.format(epoch + 1,\n",
    "                                         total_loss/num_steps))\n",
    "    print ('Time taken for 1 epoch {} sec\\n'.format(time.time() - start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
