{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Image Captioning with Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "from PIL import Image\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import plot_confusion_matrix, confusion_matrix\n",
    "\n",
    "from tensorflow.keras.layers import Dense, Flatten, Conv2D, MaxPooling2D, Dropout, BatchNormalization\n",
    "from tensorflow.keras import Model\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow import image\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "\n",
    "import itertools\n",
    "\n",
    "tf.random.set_seed(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading the Data. The data consist of pictures and labels of linear functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import os\n",
    "\n",
    "def load_images_from_folder(folder):\n",
    "    images = []\n",
    "    for filename in os.listdir(folder):\n",
    "        img = cv2.imread(os.path.join(folder,filename))\n",
    "        if img is not None:\n",
    "            images.append(img)\n",
    "    return images\n",
    "folder=\"../../linear_fcns/images\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "images = load_images_from_folder(folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = pd.read_csv('../../linear_fcns/data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_array = labels['latex'].to_numpy()\n",
    "labels_mini = label_array[:6000]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the labels we need to add start and end tokens so the model can recognize what to write."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "for val in range(len(labels_mini)):\n",
    "    labels_mini[val] = f'<start> {labels_mini[val]} <end>'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's reshape the images so we can plot them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6000, 72, 360, 1)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "images_mini = images[:6000]\n",
    "images_mini = np.array(images_mini)\n",
    "images_mini = 255 - images_mini\n",
    "images_mini = tf.image.rgb_to_grayscale(images_mini)\n",
    "images_mini = np.array(images_mini)\n",
    "images_mini.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7f934a2a4f50>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(images_mini[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can process them and prepare them for inceptionv3 which is transfer learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_mini = tf.image.resize_with_pad(images_mini, 299, 299)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7f922e07b890>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(img_mini[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = img_mini[:4000]\n",
    "y_train = labels_mini[:4000]\n",
    "X_test = img_mini[4000:]\n",
    "y_test = labels_mini[4000:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenizing the labels\n",
    "\n",
    "Now we can pad the labels to make sure they are all the same length."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the maximum length of any caption in our dataset\n",
    "def calc_max_length(tensor):\n",
    "    return max(len(t) for t in tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'tokenizer' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-30-a651cf030b41>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Choose the top 5000 words from the vocabulary\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mtrain_seqs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtexts_to_sequences\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabels_mini\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'tokenizer' is not defined"
     ]
    }
   ],
   "source": [
    "# Choose the top 5000 words from the vocabulary\n",
    "train_seqs = tokenizer.texts_to_sequences(labels_mini)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'tokenizer' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-31-6cea141d43ef>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mword_index\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'<pad>'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex_word\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'<pad>'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'tokenizer' is not defined"
     ]
    }
   ],
   "source": [
    "tokenizer.word_index['<pad>'] = 0\n",
    "tokenizer.index_word[0] = '<pad>'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the tokenized vectors\n",
    "train_seqs = tokenizer.texts_to_sequences(labels_mini)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pad each vector to the max_length of the captions\n",
    "# If you do not provide a max_length value, pad_sequences calculates it automatically\n",
    "cap_vector = tf.keras.preprocessing.sequence.pad_sequences(train_seqs, padding='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculates the max_length, which is used to store the attention weights\n",
    "max_length = calc_max_length(train_seqs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "j3j-aJRVw6Gb"
   },
   "source": [
    "Now we can begin our function to build our CNN. We do this to be able to run the CNN multiple times with different parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "f-AsfT_jw6Gb"
   },
   "outputs": [],
   "source": [
    "def build_model(conv_layers, pool_layers, epochs, X_train, y_train, X_test, y_test, layer_map):\n",
    "    cnn_model = build_cnn(conv_layers, pool_layers)\n",
    "\n",
    "\n",
    "\n",
    "      # Define a loss function\n",
    "    loss_fn = tf.keras.losses.SparseCategoricalCrossentropy()\n",
    "\n",
    "      # Compile the model\n",
    "    cnn_model.compile(optimizer='adam', loss=loss_fn, metrics=['accuracy'])\n",
    "\n",
    "    history = cnn_model.fit(X_train, y_train, epochs=epochs, validation_split=0.3)\n",
    "\n",
    "      #plot_history(history, epochs)\n",
    "\n",
    "      #confusionmatrix(model= cnn_model)\n",
    "\n",
    "    filters(model=cnn_model, X_test= X_test, y_test=y_test, layer_map=layer_map)\n",
    "\n",
    "    return history"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Q92HS9pMw6Gd"
   },
   "source": [
    "This builds the actual CNN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "qvgumgdDw6Ge"
   },
   "outputs": [],
   "source": [
    "def build_cnn(conv_layers, pool_layers):\n",
    "    model_cnn = Sequential()\n",
    "    model_cnn.add(Conv2D(30, kernel_size=5,activation='linear',input_shape=(299,299,1),padding='same'))\n",
    "    model_cnn.add(MaxPooling2D((5, 5), padding='same'))\n",
    "    model_cnn.add(Dropout(0.4))\n",
    "    x=15\n",
    "    for i in range(1, conv_layers):\n",
    "        if i <= conv_layers-1:        \n",
    "            model_cnn.add(Conv2D(x, kernel_size=5,activation='linear',padding='same'))\n",
    "            model_cnn.add(MaxPooling2D((5, 5), padding='same'))\n",
    "            model_cnn.add(Dropout(0.4))\n",
    "            x=x\n",
    "\n",
    "\n",
    "    print(model_cnn.summary())\n",
    "    return model_cnn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "BccNKnKHw6Gg"
   },
   "source": [
    "This will plot our accuracy for analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "WhJ5xISVw6Gg"
   },
   "outputs": [],
   "source": [
    "def plot_history(history, epochs):\n",
    "    epochs = range(epochs)\n",
    "\n",
    "    train_acc = history.history['accuracy']\n",
    "    val_acc = history.history['val_accuracy']\n",
    "\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    plt.plot(epochs, train_acc, color='blue', label='Training accuracy') \n",
    "    plt.plot(epochs, val_acc, color='red', label='Validation accuracy')\n",
    "    plt.legend()\n",
    "    plt.grid()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "mXpxnvpgRqdL"
   },
   "outputs": [],
   "source": [
    "def confusionmatrix(model):\n",
    "\n",
    "    confusion = confusion_matrix(y_test, np.argmax(model.predict(X_test), axis=-1))\n",
    "\n",
    "    confusion_norm = confusion.astype('float') / confusion.sum(axis=1)[:, np.newaxis]\n",
    "\n",
    "    classes=[0,1,2,3,4,5,6,7,8,9]\n",
    "\n",
    "    plt.figure(figsize=(14,14))\n",
    "    plt.imshow(confusion_norm, interpolation='nearest') \n",
    "    plt.title('Confusion Matrix')\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes)) \n",
    "    plt.xticks(tick_marks, classes, rotation=45) \n",
    "    plt.yticks(tick_marks, classes)\n",
    "    plt.xlabel('True Label')\n",
    "    plt.ylabel('Predicted Label')\n",
    "    fmt = '.2f'\n",
    "    thresh = confusion_norm.max() / 2.\n",
    "    for i, j in itertools.product(range(confusion_norm.shape[0]), range(confusion_norm.shape[1])):\n",
    "        plt.text(j, i, format(confusion_norm[i, j], fmt), horizontalalignment=\"center\",\n",
    "        color=\"black\" if confusion_norm[i, j] > thresh else \"white\")\n",
    "    plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "YemImF550a6C"
   },
   "outputs": [],
   "source": [
    "def filters(model, X_test, y_test, layer_map):\n",
    "  # redefine model to output right after the first hidden layer\n",
    "    model = Model(inputs=model.inputs, outputs=model.layers[layer_map].output)\n",
    "    model.summary()\n",
    "      # get feature map for first hidden layer\n",
    "    feature_maps = model.predict(X_test[0].reshape(1,28,28,1))\n",
    "\n",
    "    height=8\n",
    "    width=4\n",
    "    ix = 1\n",
    "    plt.figure(figsize=(14,14))\n",
    "    for _ in range(width):\n",
    "        for _ in range(height):\n",
    "            ax = plt.subplot(height, width, ix)\n",
    "            ax.set_xticks([])\n",
    "            ax.set_yticks([])\n",
    "            plt.imshow(feature_maps[0, :, :, ix-1], cmap='viridis')\n",
    "            ix += 1\n",
    "      # show the figure\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "K-ATQNhBxoOz",
    "outputId": "4bbe9a89-3a6e-4dbe-dbe8-c8aecd04c140"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_2 (Conv2D)            (None, 299, 299, 30)      780       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 60, 60, 30)        0         \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 60, 60, 30)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 60, 60, 15)        11265     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2 (None, 12, 12, 15)        0         \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 12, 12, 15)        0         \n",
      "=================================================================\n",
      "Total params: 12,045\n",
      "Trainable params: 12,045\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    },
    {
     "ename": "UnimplementedError",
     "evalue": " Cast string to float is not supported\n\t [[node sparse_categorical_crossentropy/Cast (defined at <ipython-input-32-a37d83c6d059>:12) ]] [Op:__inference_train_function_1677]\n\nFunction call stack:\ntrain_function\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mUnimplementedError\u001b[0m                        Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-37-524a20fc7e94>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mbuild_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconv_layers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpool_layers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_train\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0my_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlayer_map\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-32-a37d83c6d059>\u001b[0m in \u001b[0;36mbuild_model\u001b[0;34m(conv_layers, pool_layers, epochs, X_train, y_train, X_test, y_test, layer_map)\u001b[0m\n\u001b[1;32m     10\u001b[0m   \u001b[0mcnn_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'adam'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mloss_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'accuracy'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m   \u001b[0mhistory\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcnn_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_split\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m   \u001b[0;31m#plot_history(history, epochs)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/miniconda3/lib/python3.7/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36m_method_wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    106\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_method_wrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    107\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_in_multi_worker_mode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 108\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    109\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m     \u001b[0;31m# Running inside `run_distribute_coordinator` already.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/miniconda3/lib/python3.7/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1096\u001b[0m                 batch_size=batch_size):\n\u001b[1;32m   1097\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1098\u001b[0;31m               \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1099\u001b[0m               \u001b[0;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1100\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/miniconda3/lib/python3.7/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    778\u001b[0m       \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    779\u001b[0m         \u001b[0mcompiler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"nonXla\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 780\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    781\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    782\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/miniconda3/lib/python3.7/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    838\u001b[0m         \u001b[0;31m# Lifting succeeded, so variables are initialized and we can run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    839\u001b[0m         \u001b[0;31m# stateless function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 840\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    841\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    842\u001b[0m       \u001b[0mcanon_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcanon_kwds\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/miniconda3/lib/python3.7/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2827\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2828\u001b[0m       \u001b[0mgraph_function\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2829\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_filtered_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2830\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2831\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/miniconda3/lib/python3.7/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_filtered_call\u001b[0;34m(self, args, kwargs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1846\u001b[0m                            resource_variable_ops.BaseResourceVariable))],\n\u001b[1;32m   1847\u001b[0m         \u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1848\u001b[0;31m         cancellation_manager=cancellation_manager)\n\u001b[0m\u001b[1;32m   1849\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1850\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_flat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcancellation_manager\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/miniconda3/lib/python3.7/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1922\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1923\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0;32m-> 1924\u001b[0;31m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[1;32m   1925\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[1;32m   1926\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/miniconda3/lib/python3.7/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    548\u001b[0m               \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    549\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattrs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 550\u001b[0;31m               ctx=ctx)\n\u001b[0m\u001b[1;32m    551\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    552\u001b[0m           outputs = execute.execute_with_cancellation(\n",
      "\u001b[0;32m/opt/miniconda3/lib/python3.7/site-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0;32m---> 60\u001b[0;31m                                         inputs, attrs, num_outputs)\n\u001b[0m\u001b[1;32m     61\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mUnimplementedError\u001b[0m:  Cast string to float is not supported\n\t [[node sparse_categorical_crossentropy/Cast (defined at <ipython-input-32-a37d83c6d059>:12) ]] [Op:__inference_train_function_1677]\n\nFunction call stack:\ntrain_function\n"
     ]
    }
   ],
   "source": [
    "build_model(conv_layers=2, pool_layers=2, epochs=1, X_train=X_train, y_train=y_train, X_test=X_test, y_test=y_test, layer_map=0)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
